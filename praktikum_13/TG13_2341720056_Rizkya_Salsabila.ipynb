{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsmEwQ/E8cdFnPDgsMsSc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizkyaSalsabila/Assignment-Sem5_ML_Rizkya-Salsabila/blob/main/praktikum_13/TG13_2341720056_Rizkya_Salsabila.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 1 - JST sederhana (2 layer) dengan forward pass dan backpropagation manual"
      ],
      "metadata": {
        "id": "CFECbsUdzPue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksekusi Kode Praktikum 1"
      ],
      "metadata": {
        "id": "YnRjscII0v0Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zCs7hOshyiZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0f9d7c-222e-4be4-fb17-87e94417e4a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25275039259698706\n",
            "Epoch 1000, Loss: 0.24862955872670592\n",
            "Epoch 2000, Loss: 0.2336537841397576\n",
            "Epoch 3000, Loss: 0.08410007229585693\n",
            "Epoch 4000, Loss: 0.02020295265005958\n",
            "Epoch 5000, Loss: 0.009690546680615536\n",
            "Epoch 6000, Loss: 0.006108439769457885\n",
            "Epoch 7000, Loss: 0.004384885589915651\n",
            "Epoch 8000, Loss: 0.0033907288622123033\n",
            "Epoch 9000, Loss: 0.0027502892542091255\n",
            "Prediksi:\n",
            "[[0.04852543]\n",
            " [0.95435581]\n",
            " [0.94560466]\n",
            " [0.04276033]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset untuk masalah XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "# Label XOR (0 jika sama, 1 jika beda)\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Jumlah neuron input, hidden, dan output\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Inisialisasi bobot layer 1 dengan nilai random\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "# Inisialisasi bias layer 1 dengan nol\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "# Inisialisasi bobot layer 2 dengan nilai random\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "# Inisialisasi bias layer 2 dengan nol\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi sigmoid untuk menormalkan output 0–1\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Turunan sigmoid untuk proses backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Loop training sebanyak 10.000 iterasi\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Menghitung input ke hidden layer (z1)\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    # Menghitung aktivasi hidden layer (a1)\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    # Menghitung input ke output layer (z2)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    # Menghitung output akhir jaringan (a2)\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Menghitung error antara label dan prediksi\n",
        "    error = y - a2\n",
        "\n",
        "    # Gradien output layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    # Gradien bobot W2\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    # Gradien bias b2\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradien hidden layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    # Gradien bobot W1\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    # Gradien bias b1\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot W1, W2 dan bias b1, b2\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Cetak loss setiap 1000 epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Cetak hasil prediksi akhir jaringan\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soal Praktikum 1"
      ],
      "metadata": {
        "id": "QITzxrMS0uYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 (kode 1) : Ubah jumlah neuron hidden layer menjadi 3"
      ],
      "metadata": {
        "id": "Ehx5wYrl1JLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset untuk masalah XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "# Label XOR (0 jika sama, 1 jika beda)\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Jumlah neuron input, hidden, dan output\n",
        "input_size = 2\n",
        "hidden_size = 3   # DIUBAH SESUAI SOAL = 3\n",
        "output_size = 1\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Inisialisasi bobot layer 1 dengan nilai random\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "# Inisialisasi bias layer 1 dengan nol\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "# Inisialisasi bobot layer 2 dengan nilai random\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "# Inisialisasi bias layer 2 dengan nol\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi sigmoid untuk menormalkan output 0–1\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Turunan sigmoid untuk proses backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Loop training sebanyak 10.000 iterasi\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Menghitung input ke hidden layer (z1)\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    # Menghitung aktivasi hidden layer (a1)\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    # Menghitung input ke output layer (z2)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    # Menghitung output akhir jaringan (a2)\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Menghitung error antara label dan prediksi\n",
        "    error = y - a2\n",
        "\n",
        "    # Gradien output layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    # Gradien bobot W2\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    # Gradien bias b2\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradien hidden layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    # Gradien bobot W1\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    # Gradien bias b1\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot W1, W2 dan bias b1, b2\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Cetak loss setiap 1000 epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Cetak hasil prediksi akhir jaringan\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cfobpdU05Vq",
        "outputId": "fd5bee9b-e366-4d15-e112-1be6d5b390be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.27922107216847375\n",
            "Epoch 1000, Loss: 0.2500818117191574\n",
            "Epoch 2000, Loss: 0.2500108308056744\n",
            "Epoch 3000, Loss: 0.2499707221860559\n",
            "Epoch 4000, Loss: 0.249900957437653\n",
            "Epoch 5000, Loss: 0.24971175465134227\n",
            "Epoch 6000, Loss: 0.24898651619627613\n",
            "Epoch 7000, Loss: 0.24388634255080008\n",
            "Epoch 8000, Loss: 0.1906030395684695\n",
            "Epoch 9000, Loss: 0.05267834772304581\n",
            "Prediksi:\n",
            "[[0.1028376 ]\n",
            " [0.85365826]\n",
            " [0.8654002 ]\n",
            " [0.15068381]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 (kode 1) : Bandingkan hasil loss dengan konfigurasi awal"
      ],
      "metadata": {
        "id": "d8jXwNCF1jne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari percobaan di atas, dapat dibandingkan hasilnya bahwa :\n",
        "\" Pada konfigurasi awal (2 neuron), loss turun dengan cepat dan mencapai nilai akhir 0.00275.\n",
        "Pada konfigurasi hidden 3 neuron, loss turun lebih lambat dan hanya mencapai 0.0527. Artinya, konfigurasi awal lebih baik, karena jaringan lebih stabil dan lebih cepat menemukan pola XOR. \""
      ],
      "metadata": {
        "id": "_aJh53On2EKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 3 (kode 1) : Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
      ],
      "metadata": {
        "id": "aya2aWtR36tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Arsitektur\n",
        "input_size = 2\n",
        "hidden_size = 3   # hidden neuron = 3 (sesuai soal)\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot & bias\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# === FUNGSI AKTIVASI ===\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# --- ReLU untuk hidden layer ---\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# ============================\n",
        "# TRAINING\n",
        "# ============================\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Forward Pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)            # GANTI SIGMOID → RELU\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)         # Output tetap sigmoid\n",
        "\n",
        "    # Error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backprop Output Layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Backprop Hidden Layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(a1)   # DERIVATIVE RELU\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update Bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Print monitoring loss\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYqbFXzv4Q7-",
        "outputId": "6bcb2a22-be38-429f-c820-ad0bec950e2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25008150258581013\n",
            "Epoch 1000, Loss: 0.1668396683343036\n",
            "Epoch 2000, Loss: 0.16674472509081645\n",
            "Epoch 3000, Loss: 0.16671724962084816\n",
            "Epoch 4000, Loss: 0.1667019068793097\n",
            "Epoch 5000, Loss: 0.1666945844244845\n",
            "Epoch 6000, Loss: 0.16668899014776367\n",
            "Epoch 7000, Loss: 0.16668832080092136\n",
            "Epoch 8000, Loss: 0.16668409304457088\n",
            "Epoch 9000, Loss: 0.16668384905971667\n",
            "Prediksi:\n",
            "[[0.33342049]\n",
            " [0.33342049]\n",
            " [0.99240146]\n",
            " [0.33342049]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah menambahkan fungsi aktivasi ReLU pada hidden layer, performa jaringan meningkat. Loss yang semula stagnan pada 0.25 (menggunakan sigmoid) berhasil turun hingga 0.166. Prediksi output juga menjadi lebih mendekati nilai target XOR dibanding konfigurasi awal. Hal ini menunjukkan bahwa ReLU lebih efektif dalam mempercepat konvergensi dan menghindari vanishing gradient pada jaringan neural sederhana ini."
      ],
      "metadata": {
        "id": "R4z80AJb5EPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 2 - Penggunaan library Keras untuk JST."
      ],
      "metadata": {
        "id": "JErpJvPD5rUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksekusi Kode Praktikum 2"
      ],
      "metadata": {
        "id": "9nTouf1l63_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset Iris dari sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Untuk membagi data menjadi data train dan test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Untuk mengubah label kelas menjadi one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Import TensorFlow untuk membangun model neural network\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Load dataset Iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Menyimpan fitur (panjang/lebar sepal dan petal)\n",
        "X = iris.data\n",
        "\n",
        "# Menyimpan label kelas (0, 1, 2) dan reshape agar menjadi kolom\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# One-hot encoding label (0,1,2 → vektor 3 dimensi)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Mengubah label menjadi bentuk one-hot\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# Membagi dataset menjadi data training (80%) dan testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "# Membangun model neural network (MLP)\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    # Hidden layer pertama (10 neuron, aktivasi ReLU)\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "\n",
        "    # Hidden layer kedua (8 neuron, aktivasi ReLU)\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "\n",
        "    # Output layer (3 neuron untuk 3 kelas, aktivasi Softmax)\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Mengompilasi model menggunakan optimizer Adam dan loss categorical crossentropy\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Melatih model menggunakan data training selama 50 epoch\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "\n",
        "# Mengevaluasi performa model pada data test\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Menampilkan akurasi\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "id": "DNcbtIWe6A5N",
        "outputId": "42cc7f8e-b360-4ba1-c91d-b80612026069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3833 - loss: 2.0249\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3508 - loss: 1.6950 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7454 - loss: 1.2576 \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6573 - loss: 1.3399 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7015 - loss: 1.1248 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6865 - loss: 1.0571 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6909 - loss: 0.9583 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7117 - loss: 0.8463 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7559 - loss: 0.7272 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7426 - loss: 0.6826 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6847 - loss: 0.6826 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6607 - loss: 0.6316 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6876 - loss: 0.6110 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7423 - loss: 0.5705 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8838 - loss: 0.5413 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9403 - loss: 0.5180 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9828 - loss: 0.5053 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9333 - loss: 0.4879 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9444 - loss: 0.4727 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9506 - loss: 0.4655 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9837 - loss: 0.4412 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9630 - loss: 0.4368 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9377 - loss: 0.4116 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9415 - loss: 0.4288 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9555 - loss: 0.4041 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9364 - loss: 0.4229 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9780 - loss: 0.3696 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9574 - loss: 0.3989 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.3497 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9476 - loss: 0.3699 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9685 - loss: 0.3850 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9549 - loss: 0.3526 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9340 - loss: 0.3454 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9744 - loss: 0.3469 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.3435 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9654 - loss: 0.3640 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9484 - loss: 0.3344 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9436 - loss: 0.3225 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9846 - loss: 0.2996 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9865 - loss: 0.2790 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.2722 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9885 - loss: 0.3005 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.2837 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9939 - loss: 0.2470 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9565 - loss: 0.2510 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9885 - loss: 0.2755 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9910 - loss: 0.2669 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9844 - loss: 0.2704 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.2430 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9860 - loss: 0.2133 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.9333 - loss: 0.2866\n",
            "Akurasi: 0.9333333373069763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soal Praktikum 2"
      ],
      "metadata": {
        "id": "V3VSvNXF7ADT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 (kode 2) : Ubah jumlah neuron hidden layer"
      ],
      "metadata": {
        "id": "VKRSzdy77EjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset Iris dari sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Untuk membagi data menjadi data train dan test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Untuk mengubah label kelas menjadi one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Import TensorFlow untuk membangun model neural network\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Load dataset Iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Menyimpan fitur (panjang/lebar sepal dan petal)\n",
        "X = iris.data\n",
        "\n",
        "# Menyimpan label kelas (0, 1, 2) dan reshape agar menjadi kolom\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# One-hot encoding label (0,1,2 → vektor 3 dimensi)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Mengubah label menjadi bentuk one-hot\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# Membagi dataset menjadi data training (80%) dan testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "# Membangun model neural network (MLP) dengan jumlah neuron hidden layer yang DIUBAH\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    # Hidden layer pertama — jumlah neuron DIUBAH menjadi 6\n",
        "    tf.keras.layers.Dense(6, activation='relu', input_shape=(4,)),\n",
        "\n",
        "    # Hidden layer kedua — jumlah neuron DIUBAH menjadi 4\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "\n",
        "    # Output layer — tetap 3 neuron (3 kelas)\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Mengompilasi model menggunakan optimizer Adam dan loss categorical crossentropy\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Melatih model menggunakan data training selama 50 epoch\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "\n",
        "# Mengevaluasi performa model pada data test\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Menampilkan akurasi\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "id": "wbWVrwez7mCd",
        "outputId": "56490867-9816-4f3b-f1ea-a63296244471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3140 - loss: 1.4063\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2681 - loss: 1.2793     \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3251 - loss: 1.1303 \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4021 - loss: 1.0541 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5476 - loss: 1.0791 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6754 - loss: 1.0294 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6703 - loss: 1.0236 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5640 - loss: 1.0279 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5579 - loss: 1.0185 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4877 - loss: 1.0100 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6222 - loss: 0.9919 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5446 - loss: 0.9900 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5571 - loss: 0.9738 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6585 - loss: 0.9518 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7017 - loss: 0.9342 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7026 - loss: 0.9212 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5059 - loss: 0.9343 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5122 - loss: 0.9028 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6069 - loss: 0.8857 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6015 - loss: 0.8644 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5830 - loss: 0.8538 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5222 - loss: 0.8376 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6380 - loss: 0.8182 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5845 - loss: 0.8085 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6396 - loss: 0.7759 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6096 - loss: 0.7672 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6530 - loss: 0.7584\n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6956 - loss: 0.7283 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6231 - loss: 0.7487 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5167 - loss: 0.7475 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5746 - loss: 0.7107 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6449 - loss: 0.6937 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5931 - loss: 0.6896 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6516 - loss: 0.6794 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6104 - loss: 0.6846 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 0.6616 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6124 - loss: 0.6550 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6853 - loss: 0.6170 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6716 - loss: 0.6062 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7061 - loss: 0.5916 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6750 - loss: 0.5738 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6735 - loss: 0.5777 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6199 - loss: 0.5850 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6807 - loss: 0.5608 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6838 - loss: 0.5512 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6859 - loss: 0.5575 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6775 - loss: 0.5255 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6851 - loss: 0.5362 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7126 - loss: 0.5025 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6790 - loss: 0.5226 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.7000 - loss: 0.4993\n",
            "Akurasi: 0.699999988079071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 (kode 2) : Bandingkan akurasi dengan konfigurasi awal"
      ],
      "metadata": {
        "id": "2wC-QRUw8aR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perbandingan hasil menunjukkan bahwa konfigurasi awal model dengan jumlah neuron yang lebih besar mampu mencapai akurasi tinggi, yaitu sekitar 93%, sedangkan konfigurasi baru dengan hanya 3 neuron pada hidden layer mengalami penurunan kinerja signifikan dengan akurasi sekitar 70%. Penurunan ini terjadi karena jumlah neuron yang terlalu sedikit membuat model memiliki kapasitas belajar yang rendah sehingga pola data tidak dapat dipelajari secara optimal. Selain itu, model dengan neuron sedikit cenderung mengalami underfitting, terlihat dari loss yang masih tinggi sepanjang training. Hal ini membuktikan bahwa jumlah neuron berpengaruh langsung terhadap kemampuan representasi fitur pada jaringan saraf. Dengan demikian, konfigurasi awal tetap menjadi pilihan terbaik untuk mencapai performa prediksi yang lebih akurat."
      ],
      "metadata": {
        "id": "KTjXubVH8zQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 (kode 3) : Bandingkan Sigmoid vs ReLU pada dataset Iris"
      ],
      "metadata": {
        "id": "V9GAFbdH85Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset Iris dari sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "\n",
        "# === Fungsi untuk membuat model ===\n",
        "def build_model(activation_fn):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(4,)),\n",
        "        tf.keras.layers.Dense(10, activation=activation_fn),\n",
        "        tf.keras.layers.Dense(8, activation=activation_fn),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# === Model dengan Sigmoid ===\n",
        "model_sigmoid = build_model('sigmoid')\n",
        "history_sigmoid = model_sigmoid.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "loss_sigmoid, acc_sigmoid = model_sigmoid.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "# === Model dengan ReLU ===\n",
        "model_relu = build_model('relu')\n",
        "history_relu = model_relu.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "loss_relu, acc_relu = model_relu.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "# === Cetak hasil perbandingan ===\n",
        "print(\"=== Perbandingan Sigmoid vs ReLU ===\")\n",
        "print(f\"Sigmoid -> Akurasi: {acc_sigmoid:.4f}, Loss: {loss_sigmoid:.4f}\")\n",
        "print(f\"ReLU    -> Akurasi: {acc_relu:.4f}, Loss: {loss_relu:.4f}\")"
      ],
      "metadata": {
        "id": "1UduYMUn9gZ6",
        "outputId": "dbc9094e-2a1b-4525-e97e-f55109f37566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Perbandingan Sigmoid vs ReLU ===\n",
            "Sigmoid -> Akurasi: 0.8333, Loss: 0.6718\n",
            "ReLU    -> Akurasi: 0.9667, Loss: 0.3220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 (kode 3) : Catat perbedaan loss dan akurasi"
      ],
      "metadata": {
        "id": "roLa4PWp-ozb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil eksperimen menunjukkan bahwa penggunaan fungsi aktivasi ReLU memberikan performa yang lebih baik dibandingkan Sigmoid pada dataset Iris. Model dengan Sigmoid menghasilkan akurasi sebesar 83,33% dengan nilai loss 0,6718, yang menandakan bahwa model masih mengalami kesalahan prediksi yang cukup tinggi. Sebaliknya, model dengan ReLU mencapai akurasi 96,67% dengan loss 0,3220, sehingga terlihat bahwa model jauh lebih mampu belajar pola data dengan baik. Perbedaan hasil ini menunjukkan bahwa ReLU lebih efektif dalam menangani permasalahan non-linear dan menghindari masalah vanishing gradient yang sering terjadi pada Sigmoid. Secara keseluruhan, ReLU terbukti menghasilkan model yang lebih akurat, lebih stabil, dan memiliki loss yang lebih rendah."
      ],
      "metadata": {
        "id": "jSe611A5-tFN"
      }
    }
  ]
}