{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAQbLVMXrysef0raYwn04k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizkyaSalsabila/Assignment-Sem5_ML_Rizkya-Salsabila/blob/main/praktikum_13/TG13_2341720056_Rizkya_Salsabila.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 1 - JST sederhana (2 layer) dengan forward pass dan backpropagation manual"
      ],
      "metadata": {
        "id": "CFECbsUdzPue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksekusi Kode Praktikum 1"
      ],
      "metadata": {
        "id": "YnRjscII0v0Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zCs7hOshyiZm",
        "outputId": "7e0f9d7c-222e-4be4-fb17-87e94417e4a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25275039259698706\n",
            "Epoch 1000, Loss: 0.24862955872670592\n",
            "Epoch 2000, Loss: 0.2336537841397576\n",
            "Epoch 3000, Loss: 0.08410007229585693\n",
            "Epoch 4000, Loss: 0.02020295265005958\n",
            "Epoch 5000, Loss: 0.009690546680615536\n",
            "Epoch 6000, Loss: 0.006108439769457885\n",
            "Epoch 7000, Loss: 0.004384885589915651\n",
            "Epoch 8000, Loss: 0.0033907288622123033\n",
            "Epoch 9000, Loss: 0.0027502892542091255\n",
            "Prediksi:\n",
            "[[0.04852543]\n",
            " [0.95435581]\n",
            " [0.94560466]\n",
            " [0.04276033]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset untuk masalah XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "# Label XOR (0 jika sama, 1 jika beda)\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Jumlah neuron input, hidden, dan output\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Inisialisasi bobot layer 1 dengan nilai random\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "# Inisialisasi bias layer 1 dengan nol\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "# Inisialisasi bobot layer 2 dengan nilai random\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "# Inisialisasi bias layer 2 dengan nol\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi sigmoid untuk menormalkan output 0–1\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Turunan sigmoid untuk proses backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Loop training sebanyak 10.000 iterasi\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Menghitung input ke hidden layer (z1)\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    # Menghitung aktivasi hidden layer (a1)\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    # Menghitung input ke output layer (z2)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    # Menghitung output akhir jaringan (a2)\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Menghitung error antara label dan prediksi\n",
        "    error = y - a2\n",
        "\n",
        "    # Gradien output layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    # Gradien bobot W2\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    # Gradien bias b2\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradien hidden layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    # Gradien bobot W1\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    # Gradien bias b1\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot W1, W2 dan bias b1, b2\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Cetak loss setiap 1000 epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Cetak hasil prediksi akhir jaringan\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soal Praktikum 1"
      ],
      "metadata": {
        "id": "QITzxrMS0uYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 : Ubah jumlah neuron hidden layer menjadi 3"
      ],
      "metadata": {
        "id": "Ehx5wYrl1JLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset untuk masalah XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "# Label XOR (0 jika sama, 1 jika beda)\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Jumlah neuron input, hidden, dan output\n",
        "input_size = 2\n",
        "hidden_size = 3   # DIUBAH SESUAI SOAL = 3\n",
        "output_size = 1\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Inisialisasi bobot layer 1 dengan nilai random\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "# Inisialisasi bias layer 1 dengan nol\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "# Inisialisasi bobot layer 2 dengan nilai random\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "# Inisialisasi bias layer 2 dengan nol\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi sigmoid untuk menormalkan output 0–1\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Turunan sigmoid untuk proses backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Loop training sebanyak 10.000 iterasi\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Menghitung input ke hidden layer (z1)\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    # Menghitung aktivasi hidden layer (a1)\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    # Menghitung input ke output layer (z2)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    # Menghitung output akhir jaringan (a2)\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Menghitung error antara label dan prediksi\n",
        "    error = y - a2\n",
        "\n",
        "    # Gradien output layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    # Gradien bobot W2\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    # Gradien bias b2\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradien hidden layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    # Gradien bobot W1\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    # Gradien bias b1\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot W1, W2 dan bias b1, b2\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Cetak loss setiap 1000 epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Cetak hasil prediksi akhir jaringan\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "id": "3cfobpdU05Vq",
        "outputId": "fd5bee9b-e366-4d15-e112-1be6d5b390be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.27922107216847375\n",
            "Epoch 1000, Loss: 0.2500818117191574\n",
            "Epoch 2000, Loss: 0.2500108308056744\n",
            "Epoch 3000, Loss: 0.2499707221860559\n",
            "Epoch 4000, Loss: 0.249900957437653\n",
            "Epoch 5000, Loss: 0.24971175465134227\n",
            "Epoch 6000, Loss: 0.24898651619627613\n",
            "Epoch 7000, Loss: 0.24388634255080008\n",
            "Epoch 8000, Loss: 0.1906030395684695\n",
            "Epoch 9000, Loss: 0.05267834772304581\n",
            "Prediksi:\n",
            "[[0.1028376 ]\n",
            " [0.85365826]\n",
            " [0.8654002 ]\n",
            " [0.15068381]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 : Bandingkan hasil loss dengan konfigurasi awal"
      ],
      "metadata": {
        "id": "d8jXwNCF1jne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari percobaan di atas, dapat dibandingkan hasilnya bahwa :\n",
        "\" Pada konfigurasi awal (2 neuron), loss turun dengan cepat dan mencapai nilai akhir 0.00275.\n",
        "Pada konfigurasi hidden 3 neuron, loss turun lebih lambat dan hanya mencapai 0.0527. Artinya, konfigurasi awal lebih baik, karena jaringan lebih stabil dan lebih cepat menemukan pola XOR. \""
      ],
      "metadata": {
        "id": "_aJh53On2EKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 3 : Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
      ],
      "metadata": {
        "id": "aya2aWtR36tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Arsitektur\n",
        "input_size = 2\n",
        "hidden_size = 3   # hidden neuron = 3 (sesuai soal)\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot & bias\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# === FUNGSI AKTIVASI ===\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# --- ReLU untuk hidden layer ---\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# ============================\n",
        "# TRAINING\n",
        "# ============================\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Forward Pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)            # GANTI SIGMOID → RELU\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)         # Output tetap sigmoid\n",
        "\n",
        "    # Error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backprop Output Layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Backprop Hidden Layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(a1)   # DERIVATIVE RELU\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update Bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Print monitoring loss\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "id": "CYqbFXzv4Q7-",
        "outputId": "6bcb2a22-be38-429f-c820-ad0bec950e2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25008150258581013\n",
            "Epoch 1000, Loss: 0.1668396683343036\n",
            "Epoch 2000, Loss: 0.16674472509081645\n",
            "Epoch 3000, Loss: 0.16671724962084816\n",
            "Epoch 4000, Loss: 0.1667019068793097\n",
            "Epoch 5000, Loss: 0.1666945844244845\n",
            "Epoch 6000, Loss: 0.16668899014776367\n",
            "Epoch 7000, Loss: 0.16668832080092136\n",
            "Epoch 8000, Loss: 0.16668409304457088\n",
            "Epoch 9000, Loss: 0.16668384905971667\n",
            "Prediksi:\n",
            "[[0.33342049]\n",
            " [0.33342049]\n",
            " [0.99240146]\n",
            " [0.33342049]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah menambahkan fungsi aktivasi ReLU pada hidden layer, performa jaringan meningkat. Loss yang semula stagnan pada 0.25 (menggunakan sigmoid) berhasil turun hingga 0.166. Prediksi output juga menjadi lebih mendekati nilai target XOR dibanding konfigurasi awal. Hal ini menunjukkan bahwa ReLU lebih efektif dalam mempercepat konvergensi dan menghindari vanishing gradient pada jaringan neural sederhana ini."
      ],
      "metadata": {
        "id": "R4z80AJb5EPM"
      }
    }
  ]
}