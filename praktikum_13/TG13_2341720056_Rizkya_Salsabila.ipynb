{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpeJrFhCM+asLFYFj6RdlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizkyaSalsabila/Assignment-Sem5_ML_Rizkya-Salsabila/blob/main/praktikum_13/TG13_2341720056_Rizkya_Salsabila.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 1 - JST sederhana (2 layer) dengan forward pass dan backpropagation manual"
      ],
      "metadata": {
        "id": "CFECbsUdzPue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksekusi Kode Praktikum 1"
      ],
      "metadata": {
        "id": "YnRjscII0v0Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zCs7hOshyiZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0f9d7c-222e-4be4-fb17-87e94417e4a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25275039259698706\n",
            "Epoch 1000, Loss: 0.24862955872670592\n",
            "Epoch 2000, Loss: 0.2336537841397576\n",
            "Epoch 3000, Loss: 0.08410007229585693\n",
            "Epoch 4000, Loss: 0.02020295265005958\n",
            "Epoch 5000, Loss: 0.009690546680615536\n",
            "Epoch 6000, Loss: 0.006108439769457885\n",
            "Epoch 7000, Loss: 0.004384885589915651\n",
            "Epoch 8000, Loss: 0.0033907288622123033\n",
            "Epoch 9000, Loss: 0.0027502892542091255\n",
            "Prediksi:\n",
            "[[0.04852543]\n",
            " [0.95435581]\n",
            " [0.94560466]\n",
            " [0.04276033]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset untuk masalah XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "# Label XOR (0 jika sama, 1 jika beda)\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Jumlah neuron input, hidden, dan output\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Inisialisasi bobot layer 1 dengan nilai random\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "# Inisialisasi bias layer 1 dengan nol\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "# Inisialisasi bobot layer 2 dengan nilai random\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "# Inisialisasi bias layer 2 dengan nol\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi sigmoid untuk menormalkan output 0–1\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Turunan sigmoid untuk proses backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Loop training sebanyak 10.000 iterasi\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Menghitung input ke hidden layer (z1)\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    # Menghitung aktivasi hidden layer (a1)\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    # Menghitung input ke output layer (z2)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    # Menghitung output akhir jaringan (a2)\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Menghitung error antara label dan prediksi\n",
        "    error = y - a2\n",
        "\n",
        "    # Gradien output layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    # Gradien bobot W2\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    # Gradien bias b2\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradien hidden layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    # Gradien bobot W1\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    # Gradien bias b1\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot W1, W2 dan bias b1, b2\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Cetak loss setiap 1000 epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Cetak hasil prediksi akhir jaringan\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soal Praktikum 1"
      ],
      "metadata": {
        "id": "QITzxrMS0uYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 (kode 1) : Ubah jumlah neuron hidden layer menjadi 3"
      ],
      "metadata": {
        "id": "Ehx5wYrl1JLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset untuk masalah XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "# Label XOR (0 jika sama, 1 jika beda)\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Jumlah neuron input, hidden, dan output\n",
        "input_size = 2\n",
        "hidden_size = 3   # DIUBAH SESUAI SOAL = 3\n",
        "output_size = 1\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Inisialisasi bobot layer 1 dengan nilai random\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "# Inisialisasi bias layer 1 dengan nol\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "# Inisialisasi bobot layer 2 dengan nilai random\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "# Inisialisasi bias layer 2 dengan nol\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi sigmoid untuk menormalkan output 0–1\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Turunan sigmoid untuk proses backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Loop training sebanyak 10.000 iterasi\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Menghitung input ke hidden layer (z1)\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    # Menghitung aktivasi hidden layer (a1)\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    # Menghitung input ke output layer (z2)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    # Menghitung output akhir jaringan (a2)\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Menghitung error antara label dan prediksi\n",
        "    error = y - a2\n",
        "\n",
        "    # Gradien output layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    # Gradien bobot W2\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    # Gradien bias b2\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradien hidden layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    # Gradien bobot W1\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    # Gradien bias b1\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot W1, W2 dan bias b1, b2\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Cetak loss setiap 1000 epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Cetak hasil prediksi akhir jaringan\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cfobpdU05Vq",
        "outputId": "fd5bee9b-e366-4d15-e112-1be6d5b390be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.27922107216847375\n",
            "Epoch 1000, Loss: 0.2500818117191574\n",
            "Epoch 2000, Loss: 0.2500108308056744\n",
            "Epoch 3000, Loss: 0.2499707221860559\n",
            "Epoch 4000, Loss: 0.249900957437653\n",
            "Epoch 5000, Loss: 0.24971175465134227\n",
            "Epoch 6000, Loss: 0.24898651619627613\n",
            "Epoch 7000, Loss: 0.24388634255080008\n",
            "Epoch 8000, Loss: 0.1906030395684695\n",
            "Epoch 9000, Loss: 0.05267834772304581\n",
            "Prediksi:\n",
            "[[0.1028376 ]\n",
            " [0.85365826]\n",
            " [0.8654002 ]\n",
            " [0.15068381]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 (kode 1) : Bandingkan hasil loss dengan konfigurasi awal"
      ],
      "metadata": {
        "id": "d8jXwNCF1jne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari percobaan di atas, dapat dibandingkan hasilnya bahwa :\n",
        "\" Pada konfigurasi awal (2 neuron), loss turun dengan cepat dan mencapai nilai akhir 0.00275.\n",
        "Pada konfigurasi hidden 3 neuron, loss turun lebih lambat dan hanya mencapai 0.0527. Artinya, konfigurasi awal lebih baik, karena jaringan lebih stabil dan lebih cepat menemukan pola XOR. \""
      ],
      "metadata": {
        "id": "_aJh53On2EKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 3 (kode 1) : Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
      ],
      "metadata": {
        "id": "aya2aWtR36tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Arsitektur\n",
        "input_size = 2\n",
        "hidden_size = 3   # hidden neuron = 3 (sesuai soal)\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot & bias\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# === FUNGSI AKTIVASI ===\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# --- ReLU untuk hidden layer ---\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# ============================\n",
        "# TRAINING\n",
        "# ============================\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # Forward Pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)            # GANTI SIGMOID → RELU\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)         # Output tetap sigmoid\n",
        "\n",
        "    # Error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backprop Output Layer\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Backprop Hidden Layer\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(a1)   # DERIVATIVE RELU\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update Bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    # Print monitoring loss\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYqbFXzv4Q7-",
        "outputId": "6bcb2a22-be38-429f-c820-ad0bec950e2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25008150258581013\n",
            "Epoch 1000, Loss: 0.1668396683343036\n",
            "Epoch 2000, Loss: 0.16674472509081645\n",
            "Epoch 3000, Loss: 0.16671724962084816\n",
            "Epoch 4000, Loss: 0.1667019068793097\n",
            "Epoch 5000, Loss: 0.1666945844244845\n",
            "Epoch 6000, Loss: 0.16668899014776367\n",
            "Epoch 7000, Loss: 0.16668832080092136\n",
            "Epoch 8000, Loss: 0.16668409304457088\n",
            "Epoch 9000, Loss: 0.16668384905971667\n",
            "Prediksi:\n",
            "[[0.33342049]\n",
            " [0.33342049]\n",
            " [0.99240146]\n",
            " [0.33342049]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah menambahkan fungsi aktivasi ReLU pada hidden layer, performa jaringan meningkat. Loss yang semula stagnan pada 0.25 (menggunakan sigmoid) berhasil turun hingga 0.166. Prediksi output juga menjadi lebih mendekati nilai target XOR dibanding konfigurasi awal. Hal ini menunjukkan bahwa ReLU lebih efektif dalam mempercepat konvergensi dan menghindari vanishing gradient pada jaringan neural sederhana ini."
      ],
      "metadata": {
        "id": "R4z80AJb5EPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 2 - Penggunaan library Keras untuk JST"
      ],
      "metadata": {
        "id": "JErpJvPD5rUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksekusi Kode Praktikum 2"
      ],
      "metadata": {
        "id": "9nTouf1l63_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset Iris dari sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Untuk membagi data menjadi data train dan test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Untuk mengubah label kelas menjadi one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Import TensorFlow untuk membangun model neural network\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Load dataset Iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Menyimpan fitur (panjang/lebar sepal dan petal)\n",
        "X = iris.data\n",
        "\n",
        "# Menyimpan label kelas (0, 1, 2) dan reshape agar menjadi kolom\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# One-hot encoding label (0,1,2 → vektor 3 dimensi)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Mengubah label menjadi bentuk one-hot\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# Membagi dataset menjadi data training (80%) dan testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "# Membangun model neural network (MLP)\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    # Hidden layer pertama (10 neuron, aktivasi ReLU)\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "\n",
        "    # Hidden layer kedua (8 neuron, aktivasi ReLU)\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "\n",
        "    # Output layer (3 neuron untuk 3 kelas, aktivasi Softmax)\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Mengompilasi model menggunakan optimizer Adam dan loss categorical crossentropy\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Melatih model menggunakan data training selama 50 epoch\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "\n",
        "# Mengevaluasi performa model pada data test\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Menampilkan akurasi\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNcbtIWe6A5N",
        "outputId": "42cc7f8e-b360-4ba1-c91d-b80612026069"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3833 - loss: 2.0249\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3508 - loss: 1.6950 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7454 - loss: 1.2576 \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6573 - loss: 1.3399 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7015 - loss: 1.1248 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6865 - loss: 1.0571 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6909 - loss: 0.9583 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7117 - loss: 0.8463 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7559 - loss: 0.7272 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7426 - loss: 0.6826 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6847 - loss: 0.6826 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6607 - loss: 0.6316 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6876 - loss: 0.6110 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7423 - loss: 0.5705 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8838 - loss: 0.5413 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9403 - loss: 0.5180 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9828 - loss: 0.5053 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9333 - loss: 0.4879 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9444 - loss: 0.4727 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9506 - loss: 0.4655 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9837 - loss: 0.4412 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9630 - loss: 0.4368 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9377 - loss: 0.4116 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9415 - loss: 0.4288 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9555 - loss: 0.4041 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9364 - loss: 0.4229 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9780 - loss: 0.3696 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9574 - loss: 0.3989 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.3497 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9476 - loss: 0.3699 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9685 - loss: 0.3850 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9549 - loss: 0.3526 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9340 - loss: 0.3454 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9744 - loss: 0.3469 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.3435 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9654 - loss: 0.3640 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9484 - loss: 0.3344 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9436 - loss: 0.3225 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9846 - loss: 0.2996 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9865 - loss: 0.2790 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.2722 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9885 - loss: 0.3005 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.2837 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9939 - loss: 0.2470 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9565 - loss: 0.2510 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9885 - loss: 0.2755 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9910 - loss: 0.2669 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9844 - loss: 0.2704 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.2430 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9860 - loss: 0.2133 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.9333 - loss: 0.2866\n",
            "Akurasi: 0.9333333373069763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soal Praktikum 2"
      ],
      "metadata": {
        "id": "V3VSvNXF7ADT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 (kode 2) : Ubah jumlah neuron hidden layer"
      ],
      "metadata": {
        "id": "VKRSzdy77EjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset Iris dari sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Untuk membagi data menjadi data train dan test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Untuk mengubah label kelas menjadi one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Import TensorFlow untuk membangun model neural network\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Load dataset Iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Menyimpan fitur (panjang/lebar sepal dan petal)\n",
        "X = iris.data\n",
        "\n",
        "# Menyimpan label kelas (0, 1, 2) dan reshape agar menjadi kolom\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# One-hot encoding label (0,1,2 → vektor 3 dimensi)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Mengubah label menjadi bentuk one-hot\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# Membagi dataset menjadi data training (80%) dan testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "# Membangun model neural network (MLP) dengan jumlah neuron hidden layer yang DIUBAH\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    # Hidden layer pertama — jumlah neuron DIUBAH menjadi 6\n",
        "    tf.keras.layers.Dense(6, activation='relu', input_shape=(4,)),\n",
        "\n",
        "    # Hidden layer kedua — jumlah neuron DIUBAH menjadi 4\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "\n",
        "    # Output layer — tetap 3 neuron (3 kelas)\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Mengompilasi model menggunakan optimizer Adam dan loss categorical crossentropy\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Melatih model menggunakan data training selama 50 epoch\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "\n",
        "# Mengevaluasi performa model pada data test\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Menampilkan akurasi\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbWVrwez7mCd",
        "outputId": "56490867-9816-4f3b-f1ea-a63296244471"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3140 - loss: 1.4063\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2681 - loss: 1.2793     \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3251 - loss: 1.1303 \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4021 - loss: 1.0541 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5476 - loss: 1.0791 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6754 - loss: 1.0294 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6703 - loss: 1.0236 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5640 - loss: 1.0279 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5579 - loss: 1.0185 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4877 - loss: 1.0100 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6222 - loss: 0.9919 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5446 - loss: 0.9900 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5571 - loss: 0.9738 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6585 - loss: 0.9518 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7017 - loss: 0.9342 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7026 - loss: 0.9212 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5059 - loss: 0.9343 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5122 - loss: 0.9028 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6069 - loss: 0.8857 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6015 - loss: 0.8644 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5830 - loss: 0.8538 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5222 - loss: 0.8376 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6380 - loss: 0.8182 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5845 - loss: 0.8085 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6396 - loss: 0.7759 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6096 - loss: 0.7672 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6530 - loss: 0.7584\n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6956 - loss: 0.7283 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6231 - loss: 0.7487 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5167 - loss: 0.7475 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5746 - loss: 0.7107 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6449 - loss: 0.6937 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5931 - loss: 0.6896 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6516 - loss: 0.6794 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6104 - loss: 0.6846 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 0.6616 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6124 - loss: 0.6550 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6853 - loss: 0.6170 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6716 - loss: 0.6062 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7061 - loss: 0.5916 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6750 - loss: 0.5738 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6735 - loss: 0.5777 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6199 - loss: 0.5850 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6807 - loss: 0.5608 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6838 - loss: 0.5512 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6859 - loss: 0.5575 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6775 - loss: 0.5255 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6851 - loss: 0.5362 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7126 - loss: 0.5025 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6790 - loss: 0.5226 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.7000 - loss: 0.4993\n",
            "Akurasi: 0.699999988079071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 (kode 2) : Bandingkan akurasi dengan konfigurasi awal"
      ],
      "metadata": {
        "id": "2wC-QRUw8aR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perbandingan hasil menunjukkan bahwa konfigurasi awal model dengan jumlah neuron yang lebih besar mampu mencapai akurasi tinggi, yaitu sekitar 93%, sedangkan konfigurasi baru dengan hanya 3 neuron pada hidden layer mengalami penurunan kinerja signifikan dengan akurasi sekitar 70%. Penurunan ini terjadi karena jumlah neuron yang terlalu sedikit membuat model memiliki kapasitas belajar yang rendah sehingga pola data tidak dapat dipelajari secara optimal. Selain itu, model dengan neuron sedikit cenderung mengalami underfitting, terlihat dari loss yang masih tinggi sepanjang training. Hal ini membuktikan bahwa jumlah neuron berpengaruh langsung terhadap kemampuan representasi fitur pada jaringan saraf. Dengan demikian, konfigurasi awal tetap menjadi pilihan terbaik untuk mencapai performa prediksi yang lebih akurat."
      ],
      "metadata": {
        "id": "KTjXubVH8zQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 (kode 3) : Bandingkan Sigmoid vs ReLU pada dataset Iris"
      ],
      "metadata": {
        "id": "V9GAFbdH85Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset Iris dari sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "\n",
        "# === Fungsi untuk membuat model ===\n",
        "def build_model(activation_fn):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(4,)),\n",
        "        tf.keras.layers.Dense(10, activation=activation_fn),\n",
        "        tf.keras.layers.Dense(8, activation=activation_fn),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# === Model dengan Sigmoid ===\n",
        "model_sigmoid = build_model('sigmoid')\n",
        "history_sigmoid = model_sigmoid.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "loss_sigmoid, acc_sigmoid = model_sigmoid.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "# === Model dengan ReLU ===\n",
        "model_relu = build_model('relu')\n",
        "history_relu = model_relu.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "loss_relu, acc_relu = model_relu.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "# === Cetak hasil perbandingan ===\n",
        "print(\"=== Perbandingan Sigmoid vs ReLU ===\")\n",
        "print(f\"Sigmoid -> Akurasi: {acc_sigmoid:.4f}, Loss: {loss_sigmoid:.4f}\")\n",
        "print(f\"ReLU    -> Akurasi: {acc_relu:.4f}, Loss: {loss_relu:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UduYMUn9gZ6",
        "outputId": "dbc9094e-2a1b-4525-e97e-f55109f37566"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Perbandingan Sigmoid vs ReLU ===\n",
            "Sigmoid -> Akurasi: 0.8333, Loss: 0.6718\n",
            "ReLU    -> Akurasi: 0.9667, Loss: 0.3220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 (kode 3) : Catat perbedaan loss dan akurasi"
      ],
      "metadata": {
        "id": "roLa4PWp-ozb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil eksperimen menunjukkan bahwa penggunaan fungsi aktivasi ReLU memberikan performa yang lebih baik dibandingkan Sigmoid pada dataset Iris. Model dengan Sigmoid menghasilkan akurasi sebesar 83,33% dengan nilai loss 0,6718, yang menandakan bahwa model masih mengalami kesalahan prediksi yang cukup tinggi. Sebaliknya, model dengan ReLU mencapai akurasi 96,67% dengan loss 0,3220, sehingga terlihat bahwa model jauh lebih mampu belajar pola data dengan baik. Perbedaan hasil ini menunjukkan bahwa ReLU lebih efektif dalam menangani permasalahan non-linear dan menghindari masalah vanishing gradient yang sering terjadi pada Sigmoid. Secara keseluruhan, ReLU terbukti menghasilkan model yang lebih akurat, lebih stabil, dan memiliki loss yang lebih rendah."
      ],
      "metadata": {
        "id": "jSe611A5-tFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 3 - Penggunaan Keras untuk Regresi, khususnya pada kasus Prediksi Harga Rumah"
      ],
      "metadata": {
        "id": "-3uCBGoM_MOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksekusi Kode Praktikum 3 (1)"
      ],
      "metadata": {
        "id": "6axu8NqI_hvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas untuk membuat dan mengolah dataset\n",
        "import pandas as pd\n",
        "# Untuk membagi data train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Untuk normalisasi data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Import TensorFlow untuk membuat ANN\n",
        "import tensorflow as tf\n",
        "\n",
        "# Membuat dataset dummy berisi luas rumah dan harga rumah\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "# Mengambil kolom fitur (luas)\n",
        "X = data[['luas']]\n",
        "# Mengambil kolom target (harga)\n",
        "y = data[['harga']]\n",
        "\n",
        "# Membuat objek normalisasi\n",
        "scaler = StandardScaler()\n",
        "# Normalisasi fitur\n",
        "X = scaler.fit_transform(X)\n",
        "# Normalisasi target\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Membagi data menjadi train (80%) dan test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Membuat model ANN dengan 1 hidden layer berisi 10 neuron dan ReLU\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile model dengan loss MSE\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# Melatih model selama 100 epoch\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Menampilkan hasil prediksi pada data test\n",
        "print(\"Prediksi:\", model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7UDm4h__jBl",
        "outputId": "ab6294fa-52a8-4611-b912-04bec2f09173"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.4803\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - loss: 1.4680\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666ms/step - loss: 1.4558\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602ms/step - loss: 1.4437\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 1.4316\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 1.4196\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 1.4078\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - loss: 1.3960\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 1.3842\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 1.3726\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - loss: 1.3611\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 1.3496\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 1.3382\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 1.3270\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - loss: 1.3158\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 1.3047\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1.2936\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.2827\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - loss: 1.2719\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1.2611\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 1.2504\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1.2399\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.2294\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.2190\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2087\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1985\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.1884\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1784\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1685\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1587\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1490\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.1393\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1297\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1202\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1108\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1015\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0922\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0830\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0739\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0649\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0560\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0471\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0383\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0296\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0209\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 1.0124\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0039\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.9954\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.9871\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.9788\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.9706\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.9624\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.9543\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9463\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9384\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9305\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9227\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9150\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9073\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8997\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.8921\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.8846\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8772\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.8698\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8625\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8552\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8480\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8409\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8338\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8268\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.8198\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.8129\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8061\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.7993\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.7925\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.7858\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.7792\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.7726\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.7660\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.7595\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.7531\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.7467\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.7404\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.7341\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.7278\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.7216\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.7155\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.7094\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.7033\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6974\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.6914\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.6855\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.6797\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.6739\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.6681\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.6624\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6567\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.6511\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6455\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6399\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "Prediksi: [[0.06571806]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soal Praktikum 3"
      ],
      "metadata": {
        "id": "WAH4wiU0A99Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 (kode 4) : Ubah learning rate"
      ],
      "metadata": {
        "id": "3By5vNW6BG3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas untuk membuat dan mengolah dataset\n",
        "import pandas as pd\n",
        "# Untuk membagi data train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Untuk normalisasi data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Import TensorFlow untuk membuat ANN\n",
        "import tensorflow as tf\n",
        "\n",
        "# Membuat dataset dummy berisi luas rumah dan harga rumah\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "# Mengambil kolom fitur (luas)\n",
        "X = data[['luas']]\n",
        "# Mengambil kolom target (harga)\n",
        "y = data[['harga']]\n",
        "\n",
        "# Membuat objek normalisasi\n",
        "scaler = StandardScaler()\n",
        "# Normalisasi fitur\n",
        "X = scaler.fit_transform(X)\n",
        "# Normalisasi target\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Membagi data menjadi train (80%) dan test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Membuat model ANN dengan 1 hidden layer berisi 10 neuron dan ReLU\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile model dengan learning rate yang DIUBAH (contoh: 0.01)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss='mse'\n",
        ")\n",
        "\n",
        "# Melatih model selama 100 epoch\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Menampilkan hasil prediksi pada data test\n",
        "print(\"Prediksi:\", model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7WwlirHAlS2",
        "outputId": "b272d8f6-3049-42a7-843c-5b95dd1030e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 762ms/step - loss: 2.0780\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.9205\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.7713\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.6302\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.4998\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.3806\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.2787\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1832\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0934\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 1.0135\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.9373\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.8646\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.7959\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.7313\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.6698\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.6115\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.5562\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.5040\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.4580\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.4149\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3745\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3368\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3019\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2697\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2406\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2144\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.1906\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1693\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1503\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1336\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.1188\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1057\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0941\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0839\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0750\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0673\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0605\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0546\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0495\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0452\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0419\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0394\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - loss: 0.0373\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0357\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 0.0345\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0336\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0331\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0328\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0327\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0327\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - loss: 0.0328\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 0.0330\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0331\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0331\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0330\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0326\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0318\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0307\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0293\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0275\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0255\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0235\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0214\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0195\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0177\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0160\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0145\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0132\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0121\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0112\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0103\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0096\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0089\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0083\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0078\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0072\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0067\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0063\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0058\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0054\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0050\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0046\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0043\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0039\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0037\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0034\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0031\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0030\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0029\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0028\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0026\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0025\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0024\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0023\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0022\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0021\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0020\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0019\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0018\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0018\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Prediksi: [[0.85333675]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 (kode 4) : Bandingkan hasil loss"
      ],
      "metadata": {
        "id": "SLLzSM5FBP_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari percobaan sebelumnya, terlihat bahwa : \" ReLU lebih unggul dibanding Sigmoid pada dataset Iris karena memberikan proses training yang lebih cepat, stabil, dan menghasilkan loss akhir jauh lebih kecil. Sigmoid cenderung mengalami vanishing gradient sehingga pembelajaran berjalan lambat. ReLU mampu mempelajari pola non-linear lebih efektif, sehingga performa model meningkat signifikan. \""
      ],
      "metadata": {
        "id": "pPUC09AQBrpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksekusi Kode Praktikum 3 (2)"
      ],
      "metadata": {
        "id": "TvoV266QDyCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP regresi (Keras) menggunakan California Housing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Preprocess\n",
        "scaler = StandardScaler()\n",
        "Xs = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(Xs, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Build model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mae'])\n",
        "\n",
        "# 4. Train\n",
        "h = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "              epochs=50, batch_size=64, verbose=0)\n",
        "\n",
        "# 5. Plot training curves\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(h.history['loss'], label='train_loss')\n",
        "plt.plot(h.history['val_loss'], label='val_loss')\n",
        "plt.legend(); plt.title('MSE')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(h.history['mae'], label='train_mae')\n",
        "plt.plot(h.history['val_mae'], label='val_mae')\n",
        "plt.legend(); plt.title('MAE')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 6. Evaluasi RMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "pred = model.predict(X_val)\n",
        "rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "1tr2H6rzB3wX",
        "outputId": "fd599982-770d-44bb-d28f-05cbd720d61d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAF2CAYAAACmtO2KAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAgz9JREFUeJzt3Xl8VNX5x/HPzCQz2RcI2TCyyC6rKIho3aKoFUGroqIIbpVKq+JSaRVE/YmtVaktlYoiWBdQq2hdQIyCG5ssArLvICQhCdmXSWbu74+bTIgJkIRJZpL5vl+veWVy586dMxe9J899znmOxTAMAxERERERkVbE6usGiIiIiIiIeJsCHRERERERaXUU6IiIiIiISKujQEdERERERFodBToiIiIiItLqKNAREREREZFWR4GOiIiIiIi0Ogp0RERERESk1VGgIyIiIiIirY4CHRERERERaXUU6IjUw5w5c7BYLFgsFr799ttarxuGQUpKChaLhSuvvNKzvbCwkClTptC7d2/Cw8Np27Yt/fv359577+XgwYOe/R5//HHP8et6pKenN8v3FBGRlqGx/VKV3NxcQkJCsFgsbN68uc7PGDt27DH7pZCQEK9/JxFvC/J1A0RakpCQEN566y3OPffcGtuXLl3KgQMHcDgcnm3l5eX86le/YsuWLdx66638/ve/p7CwkJ9++om33nqLq6++muTk5BrHeemll4iIiKj1uTExMU3yfUREpGVrSL90tHfffReLxUJiYiJvvvkmTz31VJ37ORwOXnnllVrbbTbbyTdepIkp0BFpgCuuuIJ3332XF198kaCg6v993nrrLQYOHEhWVpZn24IFC1i7di1vvvkmN910U43jlJaW4nQ6ax3/2muvJS4urum+gIiItCoN6ZeO9sYbb3DFFVfQoUMH3nrrrWMGOkFBQdx8881N0naRpqahayINcOONN5Kdnc3ixYs925xOJ++9916tYGbnzp0ADB06tNZxQkJCiIqKatrGiohIq9eQfqnKvn37+Oabb7jhhhu44YYb2L17N99//31zNVmk2SjQEWmAjh07MmTIEN5++23Pts8++4y8vDxuuOGGGvt26NABgNdffx3DMOp1/JycHLKysmo8cnNzvdZ+ERFpXRrSL1V5++23CQ8P58orr2TQoEGcdtppvPnmm8f8jF/2S1lZWeTn53v9u4h4mwIdkQa66aabWLBgASUlJQC8+eabnH/++bXm24wcOZLu3bszefJkOnXqxLhx45g9ezaZmZnHPHb37t1p165djcfZZ5/dpN9HRERatvr2S1XefPNNRowYQWhoKACjRo3inXfeoaKiota+RUVFtfqldu3acf311zfdFxLxEgU6Ig10/fXXU1JSwscff0xBQQEff/xxncMDQkNDWbFiBQ899BBgVsi5/fbbSUpK4ve//z1lZWW13vPf//6XxYsX13i89tprTf6dRESk5apvvwSwfv16NmzYwI033ujZduONN5KVlcWiRYtq7R8SElKrX1q8eDHPPPNMk30fEW9RMQKRBmrXrh2pqam89dZbFBcX43K5uPbaa+vcNzo6mr/+9a/89a9/Ze/evaSlpfG3v/2Nf/7zn0RHR9ea/PmrX/1KxQhERKRBGtIvvfHGG4SHh9O5c2d27NgBmMFMx44defPNN/n1r39dY3+bzUZqamqTfweRpqBAR6QRbrrpJu68807S09O5/PLL61X+uUOHDtx2221cffXVdO7c+bjlPEVERBqiPv2SYRi8/fbbFBUV0atXr1qvZ2ZmUlhYWOcyByItkYauiTTC1VdfjdVqZfny5cccHnAssbGxnHbaaRw6dKiJWiciIoGmPv1S1do6TzzxBO+++26Nx8svv0xxcTELFixo3oaLNCFldEQaISIigpdeeok9e/YwfPjwOvf58ccfad++fa2haHv37mXTpk107969OZoqIiIBoD79UtWwtYceeoiQkJBarz/77LO8+eabWjdHWg0FOiKNdOuttx739cWLFzNlyhSuuuoqzj77bCIiIti1axezZ8+mrKyMxx9/vNZ73nvvvTqHDFxyySUkJCR4q+kiItIKHa9fKisr47///S+XXHJJnUEOwFVXXcXf//53MjMziY+PB6CiooI33nijzv2vvvpqwsPDT77hIk1EgY5IE/nNb35DQUEBn3/+OV9++SU5OTnExsYyaNAgHnjgAS688MJa7xk/fnydx/rqq68U6IiISKN98skn5ObmHjPbAzB8+HCee+455s2bxx/+8AfADJBuueWWOvffvXu3Ah3xaxajvisZioiIiIiItBAqRiAiIiIiIq2OAh0REREREWl1FOiIiIiIiEiro0BHRERERERaHQU6IiIiIiLS6ijQERERERGRVqdFrKPjdrs5ePAgkZGRWCwWXzdHRCRgGIZBQUEBycnJWK26N1ZF/ZKIiO/Ut29qEYHOwYMHSUlJ8XUzREQC1v79+znllFN83Qy/oX5JRMT3TtQ3tYhAJzIyEjC/TFRUlI9bIyISOPLz80lJSfFch8WkfklExHfq2ze1iECnalhAVFSUOhQRER/Q8Kya1C+JiPjeifomDbgWEREREZFWR4GOiIiIiIi0Ogp0RERERESk1WkRc3RExH+5XC7Ky8t93QxppODgYGw2m6+bISLSaG63G6fT6etmiBd5q29SoCMijWIYBunp6eTm5vq6KXKSYmJiSExMVMEBEWlxnE4nu3fvxu12+7op4mXe6JsU6IhIo1QFOfHx8YSFhemP5BbIMAyKi4vJzMwEICkpycctEhGpP8MwOHToEDabjZSUFC1q3Ep4s29SoCMiDeZyuTxBTtu2bX3dHDkJoaGhAGRmZhIfH69hbCLSYlRUVFBcXExycjJhYWG+bo54kbf6JoW+ItJgVXNy1LG0DlX/jpprJSIticvlAsBut/u4JdIUvNE3KdARkUbTcLXWQf+OItKS6RrWOnnj31WBjoiIiIiItDqtPtBZsSub62Z+z58+2ODrpohIK9OxY0emT5/ulWMtWbIEi8WiKnYB4pVvdnHtS9/zzqr9vm6KiLRg3uyHWqNWH+jkl1awas8RNh3M93VTRMQPXHDBBdx3331eOdaqVau46667vHIsOb4ZM2bQsWNHQkJCGDx4MCtXrjzmvhdccAEWi6XW49e//rVnn7Fjx9Z6/bLLLmuOrwLA/pxifth7hP1HipvtM0XEP6gfaj4NDnS+/vprhg8fTnJyMhaLhQULFhx3//fff59LLrmEdu3aERUVxZAhQ1i0aFFj29tgjiDzK5ZVqL66iJyYYRhUVFTUa9927dqpIEMzmD9/PhMnTmTKlCmsWbOGfv36MWzYME/p0V96//33OXTokOexceNGbDYb1113XY39Lrvsshr7vf32283xdQAICTYrCJWWu5rtM0WkZVA/5D0NDnSKioro168fM2bMqNf+X3/9NZdccgmffvopq1ev5sILL2T48OGsXbu2wY1tDHtloOOsUGciEujGjh3L0qVL+fvf/+65iz9nzhwsFgufffYZAwcOxOFw8O2337Jz505GjBhBQkICERERnHXWWXzxxRc1jvfLIQMWi4VXXnmFq6++mrCwMLp27cpHH33U6Pb+97//5fTTT8fhcNCxY0eee+65Gq//61//omvXroSEhJCQkMC1117ree29996jT58+hIaG0rZtW1JTUykqKmp0W3zp+eef584772TcuHH06tWLmTNnEhYWxuzZs+vcv02bNiQmJnoeixcvJiwsrFag43A4auwXGxvbHF/H/OzKvqm0XDfhRAKJP/dDVUOoFy1axIABAwgNDeWiiy4iMzOTzz77jJ49exIVFcVNN91EcXF1NnrhwoWce+65xMTE0LZtW6688kp27txZ49j79+/n+uuvJyYmhjZt2jBixAj27NnT6PNYXw0OdC6//HKeeuoprr766nrtP336dB5++GHOOussunbtytNPP03Xrl353//+1+DGNoYyOiJNzzAMip0VPnkYhlHvdv79739nyJAh3HnnnZ67+CkpKQA88sgjPPPMM2zevJm+fftSWFjIFVdcQVpaGmvXruWyyy5j+PDh7Nu377ifMXXqVK6//nrWr1/PFVdcwejRo8nJyWnwOV29ejXXX389N9xwAxs2bODxxx/nscceY86cOQD88MMP/OEPf+CJJ55g69atLFy4kF/96lcAHDp0iBtvvJHbbruNzZs3s2TJEq655poGnSt/4XQ6Wb16NampqZ5tVquV1NRUli1bVq9jvPrqq9xwww2Eh4fX2L5kyRLi4+Pp3r0748ePJzs7+5jHKCsrIz8/v8bjZDgqMzplugkn4jUtoS9qCf3Q448/zj//+U++//57T4Ayffp03nrrLT755BM+//xz/vGPf3j2LyoqYuLEifzwww+kpaVhtVq5+uqrcbvNv73Ly8sZNmwYkZGRfPPNN3z33XdERERw2WWX4XQ6692uxmj2BUPdbjcFBQW0adOmWT6vOqOjQEekqZSUu+g1ufmGpB5t0xPDCLPX71IWHR2N3W4nLCyMxMREALZs2QLAE088wSWXXOLZt02bNvTr18/z+5NPPskHH3zARx99xIQJE475GWPHjuXGG28E4Omnn+bFF19k5cqVDZ7/8fzzz3PxxRfz2GOPAdCtWzc2bdrEs88+y9ixY9m3bx/h4eFceeWVREZG0qFDBwYMGACYgU5FRQXXXHMNHTp0AKBPnz4N+nx/kZWVhcvlIiEhocb2hIQEz7/d8axcuZKNGzfy6quv1th+2WWXcc0119CpUyd27tzJn/70Jy6//HKWLVtW58J006ZNY+rUqSf3ZY5SPXRNfZOIt7SEvqgl9ENPPfUUQ4cOBeD2229n0qRJ7Ny5k86dOwNw7bXX8tVXX/HHP/4RgN/85jc13j979mzatWvHpk2b6N27N/Pnz8ftdvPKK694Ska/9tprxMTEsGTJEi699NJ6tasxmr0Ywd/+9jcKCwu5/vrrj7mPN++cVWV0nC51JiJybGeeeWaN3wsLC3nwwQfp2bMnMTExREREsHnz5hPeSevbt6/neXh4OFFRUcecS3I8mzdv9nQ0VYYOHcr27dtxuVxccskldOjQgc6dO3PLLbfw5ptveoYS9OvXj4svvpg+ffpw3XXXMWvWLI4cOdLgNrQGr776Kn369GHQoEE1tt9www1cddVV9OnTh5EjR/Lxxx+zatUqlixZUudxJk2aRF5enuexf//JVUurHrqmjI6ImPylHzr6/QkJCYSFhXmCnKptRx9v+/bt3HjjjXTu3JmoqCg6duwI4Gnnjz/+yI4dO4iMjCQiIoKIiAjatGlDaWlprSFu3tasGZ233nqLqVOn8uGHHxIfH3/M/bx558wRVDk8QHfNRJpMaLCNTU8M89lne8MvhzU9+OCDLF68mL/97W906dKF0NBQrr322hOm2YODg2v8brFYPOl7b4qMjGTNmjUsWbKEzz//nMmTJ/P444+zatUqYmJiWLx4Md9//71niMGf//xnVqxYQadOnbzelqYUFxeHzWYjIyOjxvaMjAzP3dBjKSoqYt68eTzxxBMn/JzOnTsTFxfHjh07uPjii2u97nA4cDgcDWv8cYR4hq6pbxLxlpbeF/lLP3T0+y0WywmPN3z4cDp06MCsWbNITk7G7XbTu3dvTzsLCwsZOHAgb775Zq3PateuXb3b1RjNFujMmzePO+64g3fffbfGWOu6TJo0iYkTJ3p+z8/P94xfbCi7MjoiTc5isdR7+Jiv2e12XK4T30X/7rvvGDt2rGc+YmFhYbNMnKzSs2dPvvvuu1pt6tatm2doVVBQEKmpqaSmpjJlyhRiYmL48ssvueaaa7BYLAwdOpShQ4cyefJkOnTowAcffFDj2toS2O12Bg4cSFpaGiNHjgTMIdBpaWnHHboB8O6771JWVsbNN998ws85cOAA2dnZJCUleaPZJxQSrIyOiLe1lL6opfRD9ZGdnc3WrVuZNWsW5513HgDffvttjX3OOOMM5s+fT3x8PFFRUc3avmYZuvb2228zbtw43n777RrrGByLw+EgKiqqxqOxqoYHuNwGFQp2RAJex44dWbFiBXv27CErK+uYd7m6du3K+++/z7p16/jxxx+56aabmiQzcywPPPAAaWlpPPnkk2zbto25c+fyz3/+kwcffBCAjz/+mBdffJF169axd+9eXn/9ddxuN927d2fFihU8/fTT/PDDD+zbt4/333+fw4cP07Nnz2ZrvzdNnDiRWbNmMXfuXDZv3sz48eMpKipi3LhxAIwZM4ZJkybVet+rr77KyJEjadu2bY3thYWFPPTQQyxfvpw9e/aQlpbGiBEj6NKlC8OGNc/d4JDK0QalyuiIBJyW0g/VR2xsLG3btuXll19mx44dfPnll7VuqI0ePZq4uDhGjBjBN998w+7du1myZAl/+MMfOHDgQJO2r8GBTmFhIevWrWPdunUA7N69m3Xr1nnG4U2aNIkxY8Z49n/rrbcYM2YMzz33HIMHDyY9PZ309HTy8vK88w1OoCqjA8rqiIg5FMBms9GrVy/atWt3zLHOzz//PLGxsZxzzjkMHz6cYcOGccYZZzRbO8844wzeeecd5s2bR+/evZk8eTJPPPEEY8eOBSAmJob333+fiy66iJ49ezJz5kzefvttTj/9dKKiovj666+54oor6NatG48++ijPPfccl19+ebO135tGjRrF3/72NyZPnkz//v1Zt24dCxcu9BQo2LdvH4cOHarxnq1bt/Ltt99y++231zqezWZj/fr1XHXVVXTr1o3bb7+dgQMH8s0333h1eNrxOCozOmXK6IgEnJbSD9WH1Wpl3rx5rF69mt69e3P//ffz7LPP1tgnLCyMr7/+mlNPPZVrrrmGnj17cvvtt1NaWtrkGR6L0cB6o0uWLOHCCy+stf3WW29lzpw5jB07lj179ngmdF5wwQUsXbr0mPvXR35+PtHR0eTl5TX4hFS43HT582cArH3sEmLD7Q16v4jUVlpayu7du+nUqRMhISG+bo6cpOP9e57M9bc1O9nzsmpPDtfNXEanuHC+evAC7zdQJACoL2rdvNE3NXgg4wUXXHDcWuG/DF6OVcGmuQTZrNisFlxuQxkdERHxC56ha8roiIg0mWYvL+0LdpvW0hER37r77rs9ZTV/+bj77rt93TxpZg4VIxCRZhaI/ZD/l6bwAkewlZJyl1agFhGfeeKJJzyFBH5JQ8ICT3VGRzfgRKR5BGI/FBCBTlVGR+sViIivxMfHH3f9MAksVeWlyypcGIbhWS1cRKSpBGI/FBBD1zzVbRToiIiIH3BULi7oNqDc1aCaQCIiUk8BEehojo6IiPgTx1FLH5RqWLWISJMIjECnciy0Ah0REfEHjiArVaPVyjRPR0SkSQREoFN150xD10RExB9YLBZP36TKayIiTSMgAh17kIauiYiIf3FUjjZQRVARkaYREIFOdUZHnYmInJyOHTsyffr0eu1rsVhYsGBBk7ZHWq4Qz1o6ugknIg3TkL4okAVUoKOMjoiI+IuQYGV0RESaUoAEOlWdiQIdERHxD1o0VESkaQVEoKM5OiIC8PLLL5OcnIzbXfNaMGLECG677TZ27tzJiBEjSEhIICIigrPOOosvvvjCa5+/YcMGLrroIkJDQ2nbti133XUXhYWFnteXLFnCoEGDCA8PJyYmhqFDh7J3714AfvzxRy688EIiIyOJiopi4MCB/PDDD15rmzQ/R7CKEYgEoubuiywWC//+97+58sorCQsLo2fPnixbtowdO3ZwwQUXEB4ezjnnnMPOnTs976lPG8rKynjwwQdp37494eHhDB48mCVLljS6nU0hMAKdqnV0XAp0RJqEYYCzyDcPo/6LLV533XVkZ2fz1Vdfebbl5OSwcOFCRo8eTWFhIVdccQVpaWmsXbuWyy67jOHDh7Nv376TPkVFRUUMGzaM2NhYVq1axbvvvssXX3zBhAkTAKioqGDkyJGcf/75rF+/nmXLlnHXXXdhqaxBPHr0aE455RRWrVrF6tWreeSRRwgODj7pdonvhGi0gYh3qS86pieffJIxY8awbt06evTowU033cRvf/tbJk2axA8//IBhGJ7+CKhXGyZMmMCyZcuYN28e69ev57rrruOyyy5j+/btjW6ntwX5ugHNoequWZnumok0jfJieDrZN5/9p4NgD6/XrrGxsVx++eW89dZbXHzxxQC89957xMXFceGFF2K1WunXr59n/yeffJIPPviAjz76qEYH0BhvvfUWpaWlvP7664SHm+395z//yfDhw/nLX/5CcHAweXl5XHnllZx22mkA9OzZ0/P+ffv28dBDD9GjRw8AunbtelLtEd9TRkfEy9QXHdO4ceO4/vrrAfjjH//IkCFDeOyxxxg2bBgA9957L+PGjfPs369fv+O2Yd++fbz22mvs27eP5GTznD/44IMsXLiQ1157jaeffrpR7fS2gMrolCmjIxLwRo8ezX//+1/KysoAePPNN7nhhhuwWq0UFhby4IMP0rNnT2JiYoiIiGDz5s1eyehs3ryZfv36eYIcgKFDh+J2u9m6dStt2rRh7NixDBs2jOHDh/P3v/+dQ4cOefadOHEid9xxB6mpqTzzzDM1hhhIy1RVjEBzdEQCT3P3RX379vU8T0hIAKBPnz41tpWWlpKfnw9wwjZs2LABl8tFt27diIiI8DyWLl3qV/1TgGV01JmINIngMPNulq8+uwGGDx+OYRh88sknnHXWWXzzzTe88MILgHk3avHixfztb3+jS5cuhIaGcu211+J0Opui5bW89tpr/OEPf2DhwoXMnz+fRx99lMWLF3P22Wfz+OOPc9NNN/HJJ5/w2WefMWXKFObNm8fVV1/dLG0T79OCoSJepr7o2M07aqhz1ZDourZVzRs6URsKCwux2WysXr0am81W47MiIiIa3U5vC4hAx175D6A5OiJNxGKpd8re10JCQrjmmmt488032bFjB927d+eMM84A4LvvvmPs2LGe4KGwsJA9e/Z45XN79uzJnDlzKCoq8mR1vvvuO6xWK927d/fsN2DAAAYMGMCkSZMYMmQIb731FmeffTYA3bp1o1u3btx///3ceOONvPbaawp0WrDq8tLqm0S8Qn2R15yoDQMGDMDlcpGZmcl5553XrG1riMAYuqaqayJylNGjR/PJJ58we/ZsRo8e7dnetWtX3n//fdatW8ePP/7ITTfdVKsqzsl8ZkhICLfeeisbN27kq6++4ve//z233HILCQkJ7N69m0mTJrFs2TL27t3L559/zvbt2+nZsyclJSVMmDCBJUuWsHfvXr777jtWrVpVYw6PtDwhmqMjEtB80RfV14na0K1bN0aPHs2YMWN4//332b17NytXrmTatGl88sknzdrW4wmIQKdqeIDumokIwEUXXUSbNm3YunUrN910k2f7888/T2xsLOeccw7Dhw9n2LBhnjtsJyssLIxFixaRk5PDWWedxbXXXsvFF1/MP//5T8/rW7Zs4Te/+Q3dunXjrrvu4p577uG3v/0tNpuN7OxsxowZQ7du3bj++uu5/PLLmTp1qlfaJr5RtcZbqRYMFQlIvuiL6qs+bXjttdcYM2YMDzzwAN27d2fkyJGsWrWKU089tVnbejwWw2hAPTwfyc/PJzo6mry8PKKiohr8/jeW7+XRBRsZdnoC/77lzCZooUhgKS0tZffu3XTq1ImQkBBfN0dO0vH+PU/2+ttaeeO8PLtoCzO+2snYczry+FWne7mFIq2f+qLWzRt9kzI6IiIiPlC9jo4yOiIiTSEgAh3N0RERb3vzzTdrlNQ8+nH66bo7Lyem8tIicrLUFx1fQFRdqxoHrUBHRLzlqquuYvDgwXW+dnTJTvGOGTNm8Oyzz5Kenk6/fv34xz/+waBBg+rc94ILLmDp0qW1tl9xxRWeSbKGYTBlyhRmzZpFbm4uQ4cO5aWXXmrWhVi1YKiInCz1RccXIIGOhq6JiHdFRkYSGRnp62YEhPnz5zNx4kRmzpzJ4MGDmT59OsOGDWPr1q3Ex8fX2v/999+vsd5EdnY2/fr147rrrvNs++tf/8qLL77I3Llz6dSpk2eF8E2bNjXbWP/qoWvqm0SkcdQXHZ+GromIiF97/vnnufPOOxk3bhy9evVi5syZhIWFMXv27Dr3b9OmDYmJiZ7H4sWLCQsL8wQ6hmEwffp0Hn30UUaMGEHfvn15/fXXOXjwIAsWLGi276WMjohI0wqIQKc6o6PORMSbWkDRRqkHf/53dDqdrF69mtTUVM82q9VKamoqy5Ytq9cxXn31VW644QbPQq27d+8mPT29xjGjo6MZPHhwvY/pDZ7y0gp0RE6KP1/DpPG88e8aEEPXlNER8a6qcb/FxcWEhob6uDVysoqLiwH/HM+dlZWFy+UiISGhxvaEhAS2bNlywvevXLmSjRs38uqrr3q2paene47xy2NWvfZLZWVllJWVeX7Pz8+v93c4luoFQ9U3iTSGzVY5B9vpVF/UCnmjbwqIQMehcdAiXmWz2YiJiSEzMxMwF7u0WCw+bpU0lGEYFBcXk5mZSUxMjOePhtbk1VdfpU+fPscsXFBf06ZN8/oCrVVV1zTaQKRxgoKCCAsL4/DhwwQHB2O1BsRApVbPm31TQAQ6yuiIeF9iYiKAJ9iRlismJsbz7+lv4uLisNlsZGRk1NiekZFxwjYXFRUxb948nnjiiRrbq96XkZFBUlJSjWP279+/zmNNmjSJiRMnen7Pz88nJSWlIV+lFpWXFjk5FouFpKQkdu/ezd69e33dHPEyb/RNARXolLnUmYh4S1UHEx8fT3l5ua+bI40UHBzs15kcu93OwIEDSUtLY+TIkQC43W7S0tKYMGHCcd/77rvvUlZWxs0331xje6dOnUhMTCQtLc0T2OTn57NixQrGjx9f57EcDgcOh+Okv0+NY2r+qMhJs9vtdO3atUalRWn5vNU3BUSg4zgqo2MYhobYiHiRzWbz6z+UpeWbOHEit956K2eeeSaDBg1i+vTpFBUVMW7cOADGjBlD+/btmTZtWo33vfrqq4wcOZK2bdvW2G6xWLjvvvt46qmn6Nq1q6e8dHJysieYag6eoWvK6IicFKvV2mxl4aVlCYhApyqjA+B0uT1zdkRExP+NGjWKw4cPM3nyZNLT0+nfvz8LFy70FBPYt29frbH5W7du5dtvv+Xzzz+v85gPP/wwRUVF3HXXXeTm5nLuueeycOHCZv1jyVOMQBkdEZEmYTFaQE2+/Px8oqOjycvLIyoqqsHvL6tw0f3RhQCsf/xSokL8r7KQiIg/Otnrb2vljfOSU+TkjCcXA7Dz6SuwWTXaQESkPup7DQ6I8hR221EZHRUkEBERP1CV0QGtpSMi0hQCItCxWCyqvCYiIn4l5Khh1Fr+QETE+wIi0AFw2Kqq26gzERER37NaLZ4RB8roiIh4X8AEOsroiIiIv6mqCqpAR0TE+wIm0NF6BSIi4m8cVSWmdRNORMTrAibQUUZHRET8jafEtDI6IiJeFzCBTtXaObprJiIi/qJ66Jr6JhERbwuYQEcZHRER8TchlUPXtGioiIj3BVygo4yOiIj4i6pAp0wZHRERrwuYQEfFCERExN9UzdFR3yQi4n0BE+ho6JqIiPibqvmjKkYgIuJ9ARPoODR0TURE/Ex1Rkd9k4iItwVMoGOvvGumjI6IiPiLEGV0RESaTMAEOlUZHadLgY6IiPiHqgVDVV5aRMT7GhzofP311wwfPpzk5GQsFgsLFiw44XuWLFnCGWecgcPhoEuXLsyZM6cRTT05nqpr6kxERMRPVK+jo4yOiIi3NTjQKSoqol+/fsyYMaNe++/evZtf//rXXHjhhaxbt4777ruPO+64g0WLFjW4sSfDbqvK6KgzERER/+ApL61h1SIiXhfU0DdcfvnlXH755fXef+bMmXTq1InnnnsOgJ49e/Ltt9/ywgsvMGzYsIZ+fKM5gpXRERER/1JVjEAZHRER72vyOTrLli0jNTW1xrZhw4axbNmypv7oGhw2zdERERH/Ul1eWn2TiIi3NTij01Dp6ekkJCTU2JaQkEB+fj4lJSWEhobWek9ZWRllZWWe3/Pz80+6HVUTPlV1TURE/IUWDBURaTp+WXVt2rRpREdHex4pKSknfcyqOToaBy0iIv4iRFXXRESaTJMHOomJiWRkZNTYlpGRQVRUVJ3ZHIBJkyaRl5fneezfv/+k21FVdU0ZHRER8RfK6IiINJ0mH7o2ZMgQPv300xrbFi9ezJAhQ475HofDgcPh8Go7qkp4qjMRERF/4dCCoSIiTabBGZ3CwkLWrVvHunXrALN89Lp169i3bx9gZmPGjBnj2f/uu+9m165dPPzww2zZsoV//etfvPPOO9x///3e+Qb15FlHRxkdERHxE9UZHfVNIiLe1uBA54cffmDAgAEMGDAAgIkTJzJgwAAmT54MwKFDhzxBD0CnTp345JNPWLx4Mf369eO5557jlVdeadbS0lB910ydiYhIyzNjxgw6duxISEgIgwcPZuXKlcfdPzc3l3vuuYekpCQcDgfdunWrMbrg8ccfx2Kx1Hj06NGjqb9GLSHK6IiINJkGD1274IILMAzjmK/PmTOnzvesXbu2oR/lVZqjIyLSMs2fP5+JEycyc+ZMBg8ezPTp0xk2bBhbt24lPj6+1v5Op5NLLrmE+Ph43nvvPdq3b8/evXuJiYmpsd/pp5/OF1984fk9KKjJR3PX4vCso6O+SUTE25r/qu4jDgU6IiIt0vPPP8+dd97JuHHjAHMh6k8++YTZs2fzyCOP1Np/9uzZ5OTk8P333xMcHAxAx44da+0XFBREYmJik7b9RDRHR0Sk6fhleemmYFcxAhGRFsfpdLJ69eoaC09brVZSU1OPufD0Rx99xJAhQ7jnnntISEigd+/ePP3007hcNa//27dvJzk5mc6dOzN69Ogaw66bS1V5aQ2rFhHxvoDJ6HiGrrnUmYiItBRZWVm4XK46F57esmVLne/ZtWsXX375JaNHj+bTTz9lx44d/O53v6O8vJwpU6YAMHjwYObMmUP37t05dOgQU6dO5bzzzmPjxo1ERkbWOmZTLGQN1cUIlNEREfG+gAl0POWlNQ5aRKRVc7vdxMfH8/LLL2Oz2Rg4cCA///wzzz77rCfQufzyyz379+3bl8GDB9OhQwfeeecdbr/99lrHnDZtGlOnTvV6W48ulGMYBhaLxeufISISqAJm6JpDGR0RkRYnLi4Om81W58LTx5pfk5SURLdu3bDZbJ5tPXv2JD09HafTWed7YmJi6NatGzt27Kjz9aZYyBqqMzqg4WsiIt4WQIGO2eGpGIGISMtht9sZOHAgaWlpnm1ut5u0tLRjLjw9dOhQduzYgdtdfb3ftm0bSUlJ2O32Ot9TWFjIzp07SUpKqvN1h8NBVFRUjYc3VM3RAY04EBHxtoAJdLRgqIhIyzRx4kRmzZrF3Llz2bx5M+PHj6eoqMhThW3MmDFMmjTJs//48ePJycnh3nvvZdu2bXzyySc8/fTT3HPPPZ59HnzwQZYuXcqePXv4/vvvufrqq7HZbNx4443N+t2CrBaslaPVSlUsR0TEqwJmjo7dZgY6LrdBhctNkC1gYjwRkRZt1KhRHD58mMmTJ5Oenk7//v1ZuHChp0DBvn37sFqrr+kpKSksWrSI+++/n759+9K+fXvuvfde/vjHP3r2OXDgADfeeCPZ2dm0a9eOc889l+XLl9OuXbtm/W4Wi4WQYBvFTpcKEoiIeFnABDqOo8ZBOxXoiIi0KBMmTGDChAl1vrZkyZJa24YMGcLy5cuPebx58+Z5q2knrSrQ0YgDERHvCpi/9u1HBTaapyMiIv4iJEglpkVEmkLABDpBNiu2yoHQumsmIiL+wlFZkKBUxQhERLwqYAIdqM7qKKMjIiL+wrPOm4oRiIh4VUAFOlXzdJTRERERfxGijI6ISJMIqECnKqOju2YiIuIvHJqjIyLSJAIr0AnS0DUREfEv1RkdBToiIt4UUIGOQ4uGioiInwnRsGoRkSYRUIGOPci8a6aMjoiI+AtldEREmkZABToODV0TERE/o9EGIiJNI6ACHbs6ExER8TNVGZ0yZXRERLwqoAIdT0bHpc5ERET8g2fomm7CiYh4VUAGOmVaq0BERPyEykuLiDSNgAp0POWlXQp0RETEP6gYgYhI0wioQMcRVDUOWoGOiIj4BxUjEBFpGgEV6NhtyuiIiIh/UUZHRKRpBFSg49CibCIi4meq5+iobxIR8aaACnSqMjplFbprJiIi/kEZHRGRphFYgY4WDBURET/jWUdHfZOIiFcFVKDjKUagzkRERPxESLDKS4uINIWACnSU0REREX+jm3AiIk0joAIdhwIdERHxM1UZnTJldEREvCqgAh17kIoRiIiIf/EUI9BNOBERrwrIQEcZHRER8RfV5aV1E05ExJsCKtDR6tMiIuJvji4vbRiGj1sjItJ6BGSgo4yOiEjLMmPGDDp27EhISAiDBw9m5cqVx90/NzeXe+65h6SkJBwOB926dePTTz89qWM2lZDKYgRuAyrcCnRERLwlwAIdszNxuhToiIi0FPPnz2fixIlMmTKFNWvW0K9fP4YNG0ZmZmad+zudTi655BL27NnDe++9x9atW5k1axbt27dv9DGbkiO4uivW8DUREe8JqEDHU4ygXIGOiEhL8fzzz3PnnXcybtw4evXqxcyZMwkLC2P27Nl17j979mxycnJYsGABQ4cOpWPHjpx//vn069ev0cdsSlWjDQBK1T+JiHhNQAU6nqFryuiIiLQITqeT1atXk5qa6tlmtVpJTU1l2bJldb7no48+YsiQIdxzzz0kJCTQu3dvnn76aVwuV6OPWVZWRn5+fo2Ht1gslqPmkCqjIyLiLQEV6FRndNSRiIi0BFlZWbhcLhISEmpsT0hIID09vc737Nq1i/feew+Xy8Wnn37KY489xnPPPcdTTz3V6GNOmzaN6OhozyMlJcUL365adUEC3YgTEfGWgAx0lNEREWm93G438fHxvPzyywwcOJBRo0bx5z//mZkzZzb6mJMmTSIvL8/z2L9/vxdbrBLTIiJNIcjXDWhOVcUINEdHRKRliIuLw2azkZGRUWN7RkYGiYmJdb4nKSmJ4OBgbDabZ1vPnj1JT0/H6XQ26pgOhwOHw3GS3+bYqjI6GromIuI9AZnRKVNGR0SkRbDb7QwcOJC0tDTPNrfbTVpaGkOGDKnzPUOHDmXHjh243dXX+m3btpGUlITdbm/UMZtaSLCK5YiIeFtABTpHr6OjRdlERFqGiRMnMmvWLObOncvmzZsZP348RUVFjBs3DoAxY8YwadIkz/7jx48nJyeHe++9l23btvHJJ5/w9NNPc88999T7mM3NM0dHGR0REa8JqKFr9qNKeDpdbs9QNhER8V+jRo3i8OHDTJ48mfT0dPr378/ChQs9xQT27duH1Vp9fU9JSWHRokXcf//99O3bl/bt23Pvvffyxz/+sd7HbG7Vc3SU0RER8ZbACnRsRwU6FQp0RERaigkTJjBhwoQ6X1uyZEmtbUOGDGH58uWNPmZz0xwdERHvC8ihawBlFbprJiIi/qHqxpsyOiIi3hNQgY7FYvFkdZwKdERExE84glVeWkTE2wIq0IGaBQlERET8QYgyOiIiXhdwgY6nxLQCHRER8ROe8tKaoyMi4jWNCnRmzJhBx44dCQkJYfDgwaxcufK4+0+fPp3u3bsTGhpKSkoK999/P6WlpY1q8MlSRkdERPyNp7y0MjoiIl7T4EBn/vz5TJw4kSlTprBmzRr69evHsGHDyMzMrHP/t956i0ceeYQpU6awefNmXn31VebPn8+f/vSnk258Y1RndHTXTERE/EN1eWn1TSIi3tLgQOf555/nzjvvZNy4cfTq1YuZM2cSFhbG7Nmz69z/+++/Z+jQodx000107NiRSy+9lBtvvPGEWaCmYldGR0RE/Ex1eWn1TSIi3tKgQMfpdLJ69WpSU1OrD2C1kpqayrJly+p8zznnnMPq1as9gc2uXbv49NNPueKKK475OWVlZeTn59d4eEtVCU91JiIi4i88c3SU0RER8ZoGLRialZWFy+WqtXJ0QkICW7ZsqfM9N910E1lZWZx77rkYhkFFRQV33333cYeuTZs2jalTpzakafWmYgQiIuJvPOvoaFi1iIjXNHnVtSVLlvD000/zr3/9izVr1vD+++/zySef8OSTTx7zPZMmTSIvL8/z2L9/v9fa4ylG4FKgIyIi/iHEs46O+iYREW9pUEYnLi4Om81GRkZGje0ZGRkkJibW+Z7HHnuMW265hTvuuAOAPn36UFRUxF133cWf//xnrNbasZbD4cDhcDSkafXmyehoeICIiPiJ6jk66ptERLylQRkdu93OwIEDSUtL82xzu92kpaUxZMiQOt9TXFxcK5ix2cwLumEYDW3vSbPblNERERH/4tCCoSIiXtegjA7AxIkTufXWWznzzDMZNGgQ06dPp6ioiHHjxgEwZswY2rdvz7Rp0wAYPnw4zz//PAMGDGDw4MHs2LGDxx57jOHDh3sCnubkqLprps5ERET8hCNY5aVFRLytwYHOqFGjOHz4MJMnTyY9PZ3+/fuzcOFCT4GCffv21cjgPProo1gsFh599FF+/vln2rVrx/Dhw/m///s/732LBlBGR0RE/E2IJ6OjQEdExFsaHOgATJgwgQkTJtT52pIlS2p+QFAQU6ZMYcqUKY35KK+rumumdXRERMRfeMpLq28SEfGaJq+65m+qMjqa8CkiIv6iqhiB5uiIiHhPwAU6yuiIiIi/cagiqIiI1wVeoGPT8AAREfEv1eWl1TeJiHhLwAU6VevoKKMjIiL+oirQcbrcuNzNv/SCiEhrFHCBTtVaBbprJiIi/qJq6BpoDqmIiLcEXKCjjI6IiPibqowOqCCBiIi3BFyg45nwqUBHRET8hM1qIdhmAZTRERHxloALdOxBKi8tIiL+p3rRUN2IExHxhoANdDR0TUSk5ZgxYwYdO3YkJCSEwYMHs3LlymPuO2fOHCwWS41HSEhIjX3Gjh1ba5/LLrusqb/GcVUtf1CqEtMiIl4R5OsGNDcVIxARaVnmz5/PxIkTmTlzJoMHD2b69OkMGzaMrVu3Eh8fX+d7oqKi2Lp1q+d3i8VSa5/LLruM1157zfO7w+HwfuMbQP2TiIh3KaMjIiJ+7fnnn+fOO+9k3Lhx9OrVi5kzZxIWFsbs2bOP+R6LxUJiYqLnkZCQUGsfh8NRY5/Y2Nim/BonFKKMjoiIVwVcoFNVjMDpUqAjIuLvnE4nq1evJjU11bPNarWSmprKsmXLjvm+wsJCOnToQEpKCiNGjOCnn36qtc+SJUuIj4+ne/fujB8/nuzs7GMer6ysjPz8/BoPb3N45ugo0BER8YaAC3RUjEBEpOXIysrC5XLVysgkJCSQnp5e53u6d+/O7Nmz+fDDD3njjTdwu92cc845HDhwwLPPZZddxuuvv05aWhp/+ctfWLp0KZdffjkuV919w7Rp04iOjvY8UlJSvPclK1VndHQjTkTEGwJwjo6GromItGZDhgxhyJAhnt/POeccevbsyb///W+efPJJAG644QbP63369KFv376cdtppLFmyhIsvvrjWMSdNmsTEiRM9v+fn53s92KlaS0c34kREvCPgMjpaR0dEpOWIi4vDZrORkZFRY3tGRgaJiYn1OkZwcDADBgxgx44dx9ync+fOxMXFHXMfh8NBVFRUjYe3eQIdZXRERLwi4AIdu83sSJTRERHxf3a7nYEDB5KWlubZ5na7SUtLq5G1OR6Xy8WGDRtISko65j4HDhwgOzv7uPs0taobcaXK6IiIeEXABTpV6xQo0BERaRkmTpzIrFmzmDt3Lps3b2b8+PEUFRUxbtw4AMaMGcOkSZM8+z/xxBN8/vnn7Nq1izVr1nDzzTezd+9e7rjjDsAsVPDQQw+xfPly9uzZQ1paGiNGjKBLly4MGzbMJ98RlNEREfG2gJujY7eZgU6F28DlNrBZa6+tICIi/mPUqFEcPnyYyZMnk56eTv/+/Vm4cKGnQMG+ffuwWqvv2x05coQ777yT9PR0YmNjGThwIN9//z29evUCwGazsX79eubOnUtubi7JyclceumlPPnkkz5dS0flpUVEvCvgAp2qjA6YWZ1Qu82HrRERkfqYMGECEyZMqPO1JUuW1Pj9hRde4IUXXjjmsUJDQ1m0aJE3m+cVnvLSGromIuIVATd0rSqjA6psIyIi/sOh8tIiIl4VcIFOkM3qGa6meToiIuIvQoJUXlpExJsCLtCB6qyOSkyLiIi/qCpGoIyOiIh3BGago7V0RETEz3jKS6sYgYiIVwRkoFPVmWjomoiI+AtldEREvCsgA53qjI7umomIiH+oKi+tvklExDsCMtBRRkdERPxNVXlpLRgqIuIdARno2D2VbdSZiIiIf/AsGKqMjoiIVwRooKOMjoiI+JeqOTrK6IiIeEdABjqeoWsudSYiIuIflNEREfGugA50NOFTRET8RdUcHZWXFhHxjoAOdDR0TURE/IUno6OhayIiXhGQgY4WDBUREX/jqbqm0QYiIl4RkIFOVWeijI6IiPgLx1EZHcMwfNwaEZGWLyADHbtNGR0REfEvVVXXQP2TiIg3BGago6FrIiLiZ0KCFOiIiHhTQAY6KkYgIiL+JthmwWoxn5ep8pqIyEkLyEDHrvLSIiLiZywWy1ElpnUjTkTkZAVkoKNiBCIi4lPpG2H1XEjfUGOzFg0VEfGegAx0NEdHRER8atkM+N8fYOtnNTZXFSQoU0ZHROSkBXSgo4yOiIj4RHwP82fmphqbqwIdZXRERE5eQAY6KkYgIiI+1a6n+TNzS43NVf1TqYoRiIictIAMdFSMQEREfKoqo5O9A1zlns0ODV0TEfGagAx0PBkdlzoSEZGWYMaMGXTs2JGQkBAGDx7MypUrj7nvnDlzsFgsNR4hISE19jEMg8mTJ5OUlERoaCipqals3769qb9GtegUsEeAuxyyd3o2hwSpGIGIiLcEdKCjO2YiIv5v/vz5TJw4kSlTprBmzRr69evHsGHDyMzMPOZ7oqKiOHTokOexd+/eGq//9a9/5cUXX2TmzJmsWLGC8PBwhg0bRmlpaVN/HZPFAu1qz9OpyuiovLSIyMkL0ECnsry0MjoiIn7v+eef584772TcuHH06tWLmTNnEhYWxuzZs4/5HovFQmJioueRkJDgec0wDKZPn86jjz7KiBEj6Nu3L6+//joHDx5kwYIFzfCNKlUNXztcPU8nRHN0RES8JiADHbsyOiIiLYLT6WT16tWkpqZ6tlmtVlJTU1m2bNkx31dYWEiHDh1ISUlhxIgR/PTTT57Xdu/eTXp6eo1jRkdHM3jw4OMe0+s8BQk2ezZ5ykurWI6IyEkL6EBHGR0REf+WlZWFy+WqkZEBSEhIID09vc73dO/endmzZ/Phhx/yxhtv4Ha7Oeecczhw4ACA530NOWZZWRn5+fk1Hictvq5ARxkdERFvaVSg05BJoQC5ubncc889JCUl4XA46NatG59++mmjGuwNKi8tItJ6DRkyhDFjxtC/f3/OP/983n//fdq1a8e///3vRh9z2rRpREdHex4pKSkn39CqQCdnF1SUAdVDq8sU6IiInLQGBzoNnRTqdDq55JJL2LNnD++99x5bt25l1qxZtG/f/qQb31gqLy0i0jLExcVhs9nIyMiosT0jI4PExMR6HSM4OJgBAwawY8cOAM/7GnLMSZMmkZeX53ns37+/oV+ltsgkcESD4YIss+JbVUZHQ9dERE5egwOdhk4KnT17Njk5OSxYsIChQ4fSsWNHzj//fPr163fSjW8szx0zdSQiIn7NbrczcOBA0tLSPNvcbjdpaWkMGTKkXsdwuVxs2LCBpKQkADp16kRiYmKNY+bn57NixYpjHtPhcBAVFVXjcdIsluqsTmVBghBP1TXdiBMROVkNCnQaMyn0o48+YsiQIdxzzz0kJCTQu3dvnn76aVyuY1/Em2Qs9FGqMzoKdERE/N3EiROZNWsWc+fOZfPmzYwfP56ioiLGjRsHwJgxY5g0aZJn/yeeeILPP/+cXbt2sWbNGm6++Wb27t3LHXfcAZgV2e677z6eeuopPvroIzZs2MCYMWNITk5m5MiRzfvl4muWmHZ4qq6pfxIROVlBDdn5eJNCt2zZUud7du3axZdffsno0aP59NNP2bFjB7/73e8oLy9nypQpdb5n2rRpTJ06tSFNaxC7rXqOjmEYWCyWJvssERE5OaNGjeLw4cNMnjyZ9PR0+vfvz8KFCz190b59+7Baq+/bHTlyhDvvvJP09HRiY2MZOHAg33//Pb169fLs8/DDD1NUVMRdd91Fbm4u5557LgsXLqy1sGiT81Re+0VGR0OrRUROWoMCncZwu93Ex8fz8ssvY7PZGDhwID///DPPPvvsMQOdSZMmMXHiRM/v+fn53pn4WckRXN0hlrsM7EEKdERE/NmECROYMGFCna8tWbKkxu8vvPACL7zwwnGPZ7FYeOKJJ3jiiSe81cTG8aylY1Zeq1owVMsfiIicvAYFOo2ZFJqUlERwcDA2m82zrWfPnqSnp+N0OrHb7bXe43A4cDgcDWlag1RldMAsSFA1lE1ERKRZxVdmmXJ2Q3lJ9YKhyuiIiJy0Bv2F35hJoUOHDmXHjh243dV3p7Zt20ZSUlKdQU5zcBwV2KjEtIiI+Ex4OwhtAxhweKsno6NiBCIiJ6/BqYyGTgodP348OTk53HvvvWzbto1PPvmEp59+mnvuucd736KBLBaLJ6ujggQiIuIzv6i8FqJiBCIiXtPgOToNnRSakpLCokWLuP/+++nbty/t27fn3nvv5Y9//KP3vkUjOIKsOF1uZXRERMS34nvC3u8gczMhHS4GdBNORMQbGlWMoCGTQsFcpXr58uWN+agmYw+yQhk4XepMRETEh9pVlZjejOO0ytEGGromInLSAnYWvmctHQ0PEBERX/IMXdusBUNFRLwoYAOdqoIEzuMsXCoiItLkqtbSyd1HKKWAhq6JiHhDwAY6yuiIiIhfCG8L4fEARBbsBJTRERHxhoANdBxBlYuyaY6OiIj4WuXCoWG52wAoVUZHROSkBWygo4yOiIj4jcrha44jZqDjchuU60aciMhJCdxAx1Y1R0cdiYiI+FhlRsees5XQyoIE2zIKfNkiEZEWL2ADHUdwZaCj4QEiIuJr8b0AsB7eytAucQCkbc70ZYtERFq8gA10qjI6ZRWa8CkiIj5WtZZO/gEu7xIKQNrmDB82SESk5QvYQMdROTRAGR0REfG50BiITALgwrY5APx4II/M/FIfNkpEpGUL2ECnOqOjQEdERPxA5cKhbYp20e+UaAC+3KLhayIijRWwgY7m6IiIiF+pWjg0czMX90wA4AvN0xERabSADXQ8VdcU6IiIiD+orLzG4c1c1MNcQPS7HVlaPFREpJECNtBxBKkYgYiI+BFPRmcLpydHkRQdQkm5i2U7s33bLhGRFirgAx1ldERExC+0627+LEzHUnLEk9X5QtXXREQapfUHOkf2wIp/w4/za2y2B6kYgYiI+JGQKIhOMZ8f3kJq5TydL7dkYhiGDxsmItIytf5AZ/8q+OxhWDWrxmZHkMpLi4iIn6laTydzM0NOa0tosI1DeaX8dDDft+0SEWmBWn+g0/4M82f6BnCVezYroyMiIn7HU5BgCyHBNs7tGgdAmqqviYg0WOsPdNp0hpBoqCiFzE2ezQp0RETE78T3Mn9mbgYgtac5Tydti+bpiIg0VOsPdCwWSB5gPv95jWezpxiBS4GOiIi/mzFjBh07diQkJITBgwezcuXKer1v3rx5WCwWRo4cWWP72LFjsVgsNR6XXXZZE7S8gY4auoZhcGFlQYL1B/LIyC/1YcNERFqe1h/oACRXDl87WB3oeDI6Wp9ARMSvzZ8/n4kTJzJlyhTWrFlDv379GDZsGJmZxx/OtWfPHh588EHOO++8Ol+/7LLLOHTokOfx9ttvN0XzG6ZdDwgKgeIs+Ol94iND6JcSA5hFCUREpP4CI9CpmqdTI6NTWYxAGR0REb/2/PPPc+eddzJu3Dh69erFzJkzCQsLY/bs2cd8j8vlYvTo0UydOpXOnTvXuY/D4SAxMdHziI2NbaqvUH/2MDj3fvP5Z49AyRFSK7M6aSozLSLSIIER6FRldDI3g7MYODqjo0BHRMRfOZ1OVq9eTWpqqmeb1WolNTWVZcuWHfN9TzzxBPHx8dx+++3H3GfJkiXEx8fTvXt3xo8fT3a2nyzMee79ENcNijJh8RQuriwz/e2OLEo1CkFEpN4CI9CJSoaIBDBckL4e0BwdEZGWICsrC5fLRUJCQo3tCQkJpKen1/meb7/9lldffZVZs2bV+TqYw9Zef/110tLS+Mtf/sLSpUu5/PLLcbnqDiTKysrIz8+v8WgyQQ4Y/nfz+Zq59HRuIDk6hNJyN9/tyGq6zxURaWUCI9CxWKqzOpXD16oyOlpHR0Sk9SgoKOCWW25h1qxZxMXFHXO/G264gauuuoo+ffowcuRIPv74Y1atWsWSJUvq3H/atGlER0d7HikpKU30DSp1OAfOGAOA5eP7uaR7DABpmqcjIlJvgRHoQPU8ncqCBHZbVXlpDQMQEfFXcXFx2Gw2MjJqzk/JyMggMTGx1v47d+5kz549DB8+nKCgIIKCgnj99df56KOPCAoKYufOnXV+TufOnYmLi2PHjh11vj5p0iTy8vI8j/3795/8lzuRS56A8HjI2spYYwEAX27OxDCMpv9sEZFWIHACnV9kdEKCldEREfF3drudgQMHkpaW5tnmdrtJS0tjyJAhtfbv0aMHGzZsYN26dZ7HVVddxYUXXsi6deuOmYk5cOAA2dnZJCUl1fm6w+EgKiqqxqPJhcbCZdMA6LhpJr3sGaTnl/LTwSYcNici0ooEUKBTuZZOzk4oycVuM6uuacFQERH/NnHiRGbNmsXcuXPZvHkz48ePp6ioiHHjxgEwZswYJk2aBEBISAi9e/eu8YiJiSEyMpLevXtjt9spLCzkoYceYvny5ezZs4e0tDRGjBhBly5dGDZsmC+/am29fwNdUrG4nLwQNgcwmP3tbmV1RETqIcjXDWg24W0hpgPk7oWDa3G0M+8EKqMjIuLfRo0axeHDh5k8eTLp6en079+fhQsXegoU7Nu3D6u1/vftbDYb69evZ+7cueTm5pKcnMyll17Kk08+icPhaKqv0TgWC/z6OfjXELqX/sj1tqW8s/YCIkOCePyq07FYLL5uoYiI3wqcQAfMeTq5e+HgGuyJQwGocBu43AY2qzoLERF/NWHCBCZMmFDna8cqIFBlzpw5NX4PDQ1l0aJFXmpZM4jtCBdMgsWP8WTYPL4sHMDcZXuxWi1MvrKXgh0RkWMInKFrUGOeTlXVNVBWR0RE/NzZv4PEPjjK8/m4/VyCqOC17/bwf59s1jA2EZFjCKxAx1N5ba1nHR1QoCMiIn7OFgQjX4LgcBKzlrPotPcBg1e+3c0zn21RsCMiUofACnSS+oPFCvk/E1R8mKrRaioxLSIifi+xD1w3ByxWTvt5AQtO/xaAf3+9i2cXbVWwIyLyC4EV6DgiIK67+fzgGhxBqrwmIiItSLdLzeIEQP+dL/HGQHNdoH8t2cnzi7f5smUiIn4nsAIdqB6+dtQ8HQU6IiLSYpx5G5x7PwDnbp7Kv84pAOAfX+7gnR+aYSFTEZEWIvACnar1dA6u8czT0RwdERFpUS6abK6x467gip8e4omzzbHYj36wkbX7jvi4cSIi/iHwAp2jMzo2s2NwuhToiIhIC2K1msUJOgyFsnxu2fUg13ez4nS5ufuN1WTml/q6hSIiPhd4gU5Cb7AGQ0kOp9qyACgrVzECERFpYYIcMOoNaNsVS/7PTCv7P05vF0xGfhl3v7FahXZEJOAFXqAT5IDE3gD0xpzEqYyOiIi0SGFt4Ob3ICwOW8YG3ur8OVEhQazZl8uUD39SJTYRCWiBF+iAZ+HQnu4dABSWVviyNSIiIo0X2xFG/guA6B9n8fqFpVgsMG/Vft5Ysc+3bRMR8aHADHQq5+n0s5oZnU82HPJla0RERE5Ot2EwcCwA/Vf/iUcvag/A1I9+YuXuHB82TETEdwIz0KnM6HRwbseKm4Ub08ks0MRNERFpwS79P4jtBPkHuK3gX1zZN4kKt8Hv3lzNwdwSX7dORKTZBWag0647BIdjKy9iePtCKtwG81dq7QEREWnBHBFw9b/BYsWyfj7P9d5Dz6QosgqdjH5lBT8r2BGRABOYgY7VBkn9ALj11GwA3lq5jwoVJRARkZbs1MGexUQdnz3Aq785hfYxoezOKuL6mcvYnVXk4waKiDSfwAx04Kh5OrtoE27nUF4pX2zO9HGjRERETtL5j0BiXyjJIXnpw7z727PpHBfOz7klXDdzGVvTC3zdQhGRZhG4gU7yAABsh9Yy6qwUAN5YvteXLRIRETl5QXa45mWwOWD75yTvnM/83w6hR2IkWYVljHp5GesP5Pq6lSIiTS5wA53KjA4ZGxl9ZiIWC3y7I4udhwt92y4REZGTFd8TLp5sPl/0Z9rt+R/vDndwQXs3ecVl3DRrhaqxiUirF7iBTmwnCI0Fl5NTCn/i4h7xALy5XGsOiIhIK3D276DjeVBeBP+9ncj/XMqc7JvZFjKOT4wJMOcK9s1/CMpVpEBEWqdGBTozZsygY8eOhISEMHjwYFauXFmv982bNw+LxcLIkSMb87HeZbHAaReZzz/4Lbf1DQHg3dX7KXZqAVEREWnhrFb4zatwxhg45SyITAaLlWDK6WDNZJBlM6dufpnDL10JpXm+bq2IiNc1ONCZP38+EydOZMqUKaxZs4Z+/foxbNgwMjOPP5F/z549PPjgg5x33nmNbqzXXfE3aNsV8vYzZPlv6RELBaUVfLTuoK9bJiIicvIiE+Cqf8AdX8ADm+HRTLhvA+VjPuXNhAfJN0Jpl/MD6S+m4ipQQR4RaV0aHOg8//zz3HnnnYwbN45evXoxc+ZMwsLCmD179jHf43K5GD16NFOnTqVz584n1WCvCmsDN78H4fFYMn7ilZDpBFPB68v2YhiGr1snIiLiXbZgiDmV4M5DufG3j/Jhv5fJMqJILN7G4RcvpDhzt69bKCLiNQ0KdJxOJ6tXryY1NbX6AFYrqampLFu27Jjve+KJJ4iPj+f222+v1+eUlZWRn59f49FkYjvC6HfBHsEpR1byN/vLbDqUx9r9uU33mSIiIj5mtVq45ZqrWHfJPH424kgsP0DRS6lk7PrR100TEfGKBgU6WVlZuFwuEhISamxPSEggPT29zvd8++23vPrqq8yaNavenzNt2jSio6M9j5SUlIY0s+GS+8P1c8FiY4T1Wx4Kms9/lqnUtIiItH6p5w4le9T/2E172hlZOF7/NVvWLPV1s0RETlqTVl0rKCjglltuYdasWcTFxdX7fZMmTSIvL8/z2L9/fxO2slKXVHMcM3BP0EdEbZxLdmFZ03+uiIiIj/Xt1QvHXYvYZutCDAWc8uH1fPHpe7jd9RzGXZAOH94DPy1o0naKiDREgwKduLg4bDYbGRkZNbZnZGSQmJhYa/+dO3eyZ88ehg8fTlBQEEFBQbz++ut89NFHBAUFsXPnzjo/x+FwEBUVVePRLAaMhgv/DMBk62t88d9X2J9TrPk6IiI+5u1qn4ZhMHnyZJKSkggNDSU1NZXt27c3QctbjuTkFJLv/YLNIQOIsJRy3oq7efbF6WzPKDj+G3P3w2uXw9o34L3bYN+K5mmwiMgJNCjQsdvtDBw4kLS0NM82t9tNWloaQ4YMqbV/jx492LBhA+vWrfM8rrrqKi688ELWrVvX9EPSGuNXD7Hz1GuxWQyu2/Vn3nnu9/R9fCHXvvQ9jy7YwBvL97Jydw57soooKC1XECQi0sSaotrnX//6V1588UVmzpzJihUrCA8PZ9iwYZSWljbV12gRIqJi6TbxM/bEXYDDUs7EI08y4x9/4W+LtlJa7qr9hpzd8NoVkLMLLDYwXPDeOCjKbv7GH0/WDjiyx9etEJFmZjEa+Jf6/PnzufXWW/n3v//NoEGDmD59Ou+88w5btmwhISGBMWPG0L59e6ZNm1bn+8eOHUtubi4LFiyo92fm5+cTHR1NXl5es2R3nE4n61++gzOzPgTgK1c/7i//HblE1trXHmQlLtxO2wgHbSPs/LpPEted6YcBnIhIIzT39bcugwcP5qyzzuKf//wnYN5gS0lJ4fe//z2PPPJIne9xuVz86le/4rbbbuObb76p0e8YhkFycjIPPPAADz74IAB5eXkkJCQwZ84cbrjhhhO2yR/OS5NylVP0zl2Eb30ft2HhTxW3syzmSp4a2ZvzurYz98naDnOHQ8EhaHMa3Pg2zLsJsndAl0vgpnfMtXx87cgemHE2BIfAfRvAUbsvF5GWpb7X4AZfgUaNGsXf/vY3Jk+eTP/+/Vm3bh0LFy70FCjYt28fhw4danzL/YDdbufMCa/DiH9hBIVwoe1Hvo+dyuMDnVzQvR2ntgkj3G4DwFnh5mBeKRt+zmPJ1sM88v4GdmQW+vgbiIi0Dk1R7XP37t2kp6fXOGZ0dDSDBw8+5jGbtRqoP7AFEz7qVYyBt2G1GDwT/AqX5L7LLa+u5N55a8ncscYcrlZwCNr1gHGfQrvucN1cCAqBHYvhu+m+/hamJc9ARQmUHIGN7/u6NSLSjIIa86YJEyYwYcKEOl9bsmTJcd87Z86cxnykbwwYjSWpL8y/hbAjuxm75S7GXv4XGDgOLBZKnC6yi8rILnSSXVTGK9/s5vud2fxl4RZmjTnT160XEWnxjlftc8uWLXW+p6ra57p16+p8vapKaEMqiE6bNo2pU6c2sPUtnNWK5crnISQSvvs7jwa/SZSlhEU/DiR48zSwFOKK74Pt1gUQXllwKLE3XPEsfPR7+PJJSBkMHYf67jtkboEf51X/vvY/MPBW37VHRJqVH+SU/VxiH7hrCXT/Nbic8PH98MFvYf9KQq0uTokNo19KDBf1SOCJEb2xWS0s3pTBil1+Nj5ZRCQANLba54n4pBqoP7BY4JIn4OLJAPwh6H0+dEwh1lLIOvdpXJT1AHPWFeCscFe/Z8At0O9GMNxmcYLCwz5qPPDV/wEGdDwPrEFwYBVkbPJde0SkWSnQqY/QGLjhTUidChYrrJ8Pr14Cz5xqjk9e8gzs/oYusTZuOMucn/P0p5vrX5ZTRETq1BTVPqveV99jgg+rgfqL8x6Ay58FIIgKjsQN5LGop9hbbOfx/23i0heW8umGQ2aBHosFfv2cOaStMB3evxPcdRQyaGo/r4HNHwEWM8vU7TJz+9r/NH9bRMQnFOjUl8UC594HYz+BHldCaBtzzO/ur2HJNJh7JUxL4dGKGcTY3fx4II+PN7TsuUoiIr7WFNU+O3XqRGJiYo1j5ufns2LFijqPKZUG3wU3zoNfPUzsXf/jg/sv46mRvYmLsLMnu5jfvbmGYdO/5pVvdpHtDDLn6wSHwa6v4Jvnmr+9Xz5l/uw7CuJ7whmVQ9Z+fBsqtE6eSCBocNU1X/DL6jZuN2Rtg73fwd7vzZ8FZmCzN3YIww7dRVxsDGkPnI8jyObjxoqINI4/XH+botrnX/7yF5555hnmzp1Lp06deOyxx1i/fj2bNm0iJCTkhG3yh/PiLwrLKnh56U5mfbObksoS1ME2Cxf3SOAPcavpteIhc8foFHM4+NGPmA7mjUQAwzCHiDuLzIfhgohEs1paQ+35DuZcYQ5Xm/ADtOlkZpVe6A0FB+Ha2dD7N146AyLS3Op7DW5UMQLBLJkZ38N8nHW7eYHekQbv3EKHI8t4KzSPMUcm8p9le7njvM6+bq2ISIs1atQoDh8+zOTJk0lPT6d///61qn1aG1jG+OGHH6aoqIi77rqL3Nxczj33XBYuXFivIEdqinAEMfHS7tx+Xmf+9+NB3vlhP+sP5LHwp3QW0p4nw0Zwi/tDyNtvPrZ+Wv1mRzTYw8BZDOVF4K6o4wMSzCApJqXy56nQ+QKI61p3gwzDLIQAZhanTSfzudVmLgz+9bOw5j8KdEQCgDI63rZvObx5HZTls87dmd9bH+Xjh68iOizY1y0TEWmwFnX9bUY6L8e3+VA+7/5wgA/WHuBIcTmRFNPTspfhCVlcGJ1B+7IdWDI3g7u87gPYHGamp+IYC7hag+HCP8HQe80A5mjbF8Ob15plrv+wDqKSql87sgf+3s98fu+PENvxJL+piPhCfa/BCnSawsG1GP+5BktJDpvdKSw6Yyb3jTzX160SEWmwFnf9bSY6L/XjrHDzxeYM5q3azzfbD1P1F0dchIMbzkjg5i5OEiODIDjczOzYw83ntiAzM1OcA3n7ILcyG5S7Dw6th33fmwdKORuunlmdtXG74eXzIX09nPN7uPSp2o16fQTsWgK/ehgu+nOznAcR8S4FOr6WuZmy167CUZLJbiMRx20fk9zhGGl2ERE/1SKvv81A56Xh9ucU8/bKfbzzwwGyCs1iABYL9EyMomdSFD2TIumRGEWPpEjiIhzHPpBhwLq34LM/grMA7BFw2TSzrPWmD+HdW8EeaWZswtvWfv/G/5plryOT4f6NtTNCIuL3FOj4ASN7J9n/uow4VybZQQm0ver/oMcV5h0rEZEWoKVef5uazkvjVWV53l65j2+2Z9W5T1yEg97to7iqXzJX9EkiJLiOYOTIXvjg7ursTvcrIGs7ZG+HCybBBY/U3YCKMniuO5QcgZvehW6XeumbiUhzUaDjJ7Zu3Uzwm1fT2WpWZDOCw7H0vBL6Xg+dLjDT8+K/nEWw4V04/RoIaVn/7Yl4Q0u+/jYlnRfvOJRXwvoDeWw5VMCW9Hy2pBewJ7uIo/8yiQoJ4uoB7blh0Kn0TPrFuXa7YNk/zVLSLqe5LbSNmc053jV74SRY/i/oORxGveH9Lyb+6che+PAeGHAz9LvB162Rk6BAx488Pu9rYje+xkjrt3SwZla/EB4Pfa411+VpfwYEh/qukVK3/90Hq1+DvjfANf/2dWtEml1Lv/42FZ2XplPsrGBbRiFfbzvMOz/s58CREs9r/U6J5oZBp3JFnySiQ48q8pO+Ed6/CzJ/Mhc2HXzX8T8kYxO8NMQsPz1xM0TEN9G3Eb8y/xZzEdmgEBj/PbQ9zdctkkZSoONHnBVu3li+l5eW7OCUop8YafuWEUHLiaGgeidrECT2hZRBcMpZ5s/olOr1BaT5FWTA9D7gKjP/fe79EaJP8XWrRJpVS7/+NhWdl+bhdht8tzOLeSv38/mmdMpd5p8sFgt0T4jkrI5tOLNjLGd2bEP7yCDzjn1cl/odfNbF8PMPcMkTZvW2k3VkL3z7vFlA4YJHIOH0kz+meM/+VfBqavXvHc+DW/+nv7NaKAU6fqi03MXbK/fx0pKd5BQUcZ51AzeGLGdo0GbCnXWMU45Mgg7nQIeh5v+QcV31P2RzSnui5mreQybAsP/zXXtEfKC1XH+9Teel+WUVlvHBmp9554f9bM8srPV6cnQIfU6JxmqxUFbhpqzCRVm52/M8JsxO/5QY+qfE0C8lhuSd72D53x+gbRdzUVGLBcpL4chuyN4JObvMSnDdLofo9sduWN7P8M3fzLV5qsplW6xwxhi48M/KFvkDw4A5V8Leb6HLJeYi7+XFcNU/4YxbfN06aQQFOn6stNzFvJX7eGnpTjLyywCDUyxZnB+2m8ui9tHb2EpM/lYsv1w4LTweOg6tDnzadVfg01TKCuGFXlCaBwPHwuo5ZmWf+3+C0BgfN06k+bS266+36Lz4VmZBKav3HGHVniOs3pvDxoP5uNwN+3OmQ4SbRa47CDFKyWs3kMiyTKz5B4A6jtN+oDnMvOfw6oVKCzLg2xfgh9lm5h+g84Xm3KBNH5q/2yPhvIlw9u8guBUtRut2wfcvms8HjKm7up0/qVpbyeaAP6yBnxbA53+GkGi4ZxVEJvi6hdJACnRagNJyF/9dc4DFmzJYviub0nK357UQyhged4irY/fQ372RsIw1tRdOi0gwA55OvzIfsR0V+HjLsn/BoknQ5jSYsApmnguZmyD1cTj3fl+3TqTZtNbr78nSefEvxc4K1u3LZWtGAUE2K46gqocNR7AVu83Kz7klrNufy4/7c9mSXoDLbTAtaBY3Bn1V41gl1nAKw0/F2rYLMRWHsR1YQY3gp10PSB5gBjPlxea2U8+Bix41b0YC7F1m9iEH15q/R58KlzxuFrZp6n7a7YLyEjO75KowizS4y8FVbt6wO3oB1UYd3w3/+wOs/Y/5e1CIObH/7N+ZN2D9jdsFM88z529Vra3kqjCHsR1cC71GwvVzfd1KaSAFOi1MWYWL1XuO8M2OLL7ZfpiNP+fXeL1PQghjO2ZzUch2YjNXwP4VtQOf6BQz8Ensbabi23aBmFPBFow0gKscXhxgLk535XQ4cxysexsW3A0RiXDfegg6zhoPIq1IIFx/G0PnpWUrcbr46WAem3ftJWLzfLbkBfFDQRv2GIlkEwWYwUiY3caoHnbGtN1Ex8wvsexeCkePtmh/prnoaOcLawcwbrdZtTNtKuT/XLn/QEidCp3OO7kvYBhQmAHZOypLau+ofhzZU7ONv9RrJKROgTadG/e5nz4Eq2aZw/Pa9TQDiCpdUs2A57SL/OfG64/z4IPfgiMa7l0HYW3M7ekb4N/ng+GCG96CHr/2aTOlYRTotHDZhWUs2XqYj9cf5JvtWVQclZLv3T6K1K4xnGHbQdeiNbTLWkHQwTXVY4OPZg0yMz1tu0JMCoTEmKnakGhzCFbV86AQMyCyBoPNbj63BUNwWOAtprb+HXj/TghvB/dtNIcbVDjh7/2g4KDG9EpACcTrb33ovLQ+ucVO1h/IY/2BXNYfyOPHA7mVw8tNHduGcVO/aK6L2kTskY3Q+QLoNuzEf9A7i80S2N9Oh/Iic1uXS8xgI7HPiRtWmg+ZmyFjozmyIGOTGVyU5tXvi3n69SDzWBjmtrNuh189XP9hZ4YBix+D7/8BWODqmdB3FOxbBstmwJZP8GS+4nvBNbPMG6++VFEG/zgT8vbVPSLji6lmAYnIJLhnhfn3kLQICnRakdxiJ4t+Sufj9Yf4fmd2neOQE0NdXBa1l3OCt9EzOIPEigMEH9kFFSV1HLEBgkIgqZ95F6r9QDjlTIjpUH1hd1WYd5AyNlY+fjIzTX2uN0tnt7SS2YZhprgzNpjDEH71UPVr371oXuTjusPvloPV6rt2ijSTQL/+HovOS+tnGAar9x7h3R8O8PH6gxQ5XQBYLTC0Sxwd2oYREmQjJNhGqN2GI8hKSLCN9rGhDD0tDnvQL/qIwkxY+ldzyQJ3BWAx19S78E/mDUmAoiw4uA4OVT1+hNx9dTfQYjX747iulaM4TjNvarY9zVxLyBZs3uw8OhDL+AkWT4Edi83fHVHmHKLBd5+4v/7qaVj6F/N51WiHo+XsghX/NosylBeZo0zuWgLhccc/blOqGoYemQS/X2MWlzhaeQm8NBRydsKZt8GVL/imndJgCnRaqezCMhb+lM7Gn/PYnVXE7qyiGnecjnZ6YgS/7gQXxRfQ1XoIW1EGlOSad4FKK39W/e4qM4dsucqrJ1UeS1icGfwUZ8PhLbWH0FUJjTWrzpx5O8R2OJmv3Xx2pMEb10BwONy/sTrFDeadsBdOh7J8uHE+dL/Md+0UaSa6/tZN5yWwFDsr+HRDOu/8sJ+Vu3NOuH9MWDBX9k1iZP/2DOwQi+XoYCN7J3z1f7Dxv+bv1mDoeK45BC3/QN0HjEyGhF5myer4083ncd0aP4x61xL4/FFz+BZA1Clw9t3mELyE02tnqb553hyCB3DZX8x9j6UoG169xAweOv0Kbv7AN4ujl+bB3/tDSQ4MfxEG3lr3fnu+hTmVw9bGfWZWu/U32xfDD6+ZN2ATevm6NTWVl8JPH5jDMZtxCQ4FOgGk2FnBnqxi9mQXsSW9gK+3HebHA7m1VpYecGosbSPstAmzExtup024ndgwO20j7HRpF0FsuN3c2TDMyXsuJ+QfNNcZ+Hk1HPjBvCj+cohccLh5YUw43UxTl+ab/0PmVd2FskD3y2HQndDpAv/OhMy9CnYvhcHj4fJnar/++WNmpZkOQ2Hcp83fPpFmputv3XReAtfe7CK+2JxJfkk5pRUuSp0uSsvdlJS7KCl3sW5/LocLqm8YprQJZUS/9owckEyHtuEEWS1m4HNwrTl0alfNYgi07QJJ/SG5v3lTMaF3zZtumNmm/JIK8/PLXWYJ7XKzjLbT5eb0pGiiw04wP9fthg3vQNqTNQOs8HZmgNL5Auh0vjkkbdEk87X6FuTJ3GyuU1Re5LulGb58Cr5+1gwIxy87frD10R9gzVwzI3bHF/5VXXXPt/Cfq82/ydp0ht9+DY5IX7fKZBjw7ljYtMAskHXr/5qtIIUCnQCXXVjG19sPs2TrYZZuO0xucR3zd36hfUwovdtH0Ts5mt6nRNM7OZp2kb+4W1Reag5RS19vZnYSTofYTmC1YhgG+3NKyCoqo0d8GGF70mDlyzUv4ja7mWqP7UhZ1KnsKI9jWU4kuyvaMuzsAfyqX3ffBUIH18HL54PFZk5YjDm19j75B2F6XzPYuyPNHMon0orp+ls3nRc5Fpfb4PudWSxYe5CFGw95hrwdzW6zYg+yEmyzMNi6mT5BP2NNPJ02Xc6kd+dT6J4QSZCtui90uw22pBewcnc2K/fksHJ3DlmFzmO2ITIkiPtSuzFmSAeCbSfoU8tLzQpq2xbC3u+rK8n90vmPwIWT6nUOALOE87uVWZTfvGoOZ28uBelmUaHyYhj1JvS88vj7l+TCjEFmgYc2nWHUG/6x4GvGTzD7cijLwyyQYZjzoq552dctM33znLnmYJXweDPYie/R5B+tQEc8XG6DHw/ksiOjkJxiJ0eKnOQUOTlS7CS7yElmfhk/59Y9lychykH3xCh6JkbSPTGSHolRnBYfjiPIhrPCzcaDeazec4TVe4/ww94jZBWad7GsFuiWEMmAU2M4LzaHs7MXELvtPSzOguO2tQIbhMcTFJ1oVjiLTAAsUFZgPpyF5tCxsgIz6xTb0bx7ENfdHKfcrrs5Frcx1V7eu80cStDnevjNrGPvt+B3sO5N6HkVjPpPwz9HpAXR9bduOi9SHyVOF19szmDB2p9Zuu1wjcJCxxMSbKVP+2hOT47mwJFiVu7OIb+0diU1m9VCSJAVR7DNU1K7rMLNoTxzSPlp7cKZPPx0zu/Wrn4NrnDCgVXmyIZdS8yRHIYLzvkDXPJEw/vWxVPgu+kQFAp3LK5f8YVfcrvMOU3HG6ZnGGZlu8Nb4PA22PKxuSjoKYPg9s/r1+5DP8K8m83RKMFhcNU/mjc4+6Xc/eYQwIJDZvny8x82h9Ybbhg5E/rf6Lu2AWxdCG/fABhmpm/jf81RP2FxZrBTnyF2htHo6nwKdKRB8krK2XQwn58O5rHh5zw2/pzHrqwi6vqvw2a1cGqbMH7OLcFZ4a7xWrDNQnSo3RPwHC0kCNpWHOZUayanWjI51ZLB6SE5dHdkE1V2iPCKXO98GXukeSem/UBof4b580RrDB3ZAy+eYV7Q7/72+BfjzM3wr7MxsFBxzw8Et+ty8m0uzTPbYHOYQxRCY1t2WXBnMWx8D7BArxHmAnrSIun6WzedF2koZ4WbEqc5tMzpclNeYf50VgYmP+7P9azzU1BWO6gJt9sY2LENgzu1YVCnNvRpH01IcO2qqC63wTs/7OfZRVvJKTKzPqk94/nzr3vRKS68YY0uKzBHMjR2OJLbZS7UufNLczTHXUtqDcOrbng55OyuDFa2Vv/M2mbOHQ4KMSvHhsZUV5C1h5nFGg5vg1o3Ui0w9pPqtY3qozjHvOlZNRJl8Hi49Mnm74+Lc2D2MPO7t+sJt31m/l2w9K/m/K7gcHMIW5wX/v5ojMPbYNZF5jmvKuJQnAOvj6gc8dO2MtipIyvmdsGOL2DVq5ByVs2iTw2gQEdOWlFZBVvSC9iaXsCW9HzP87yS6mFwbcLtnHFqLGd2jGVgh1jPhTc9r5R1lRftdfuPsOFAHkVOF0FWC4M7t+HiHglc3DOeDm2rL7o7DuXw4offsWfvbuItuXQPL+KarkF0jo/E4og0x6Q6Is1AxhFpBi7ZO8wLweFtkLXVvEgatYcJENrGDHiS+pkXyaAQ845NcIh5p+mn9831Dk67CG754Ljnxe02yJh5FUmZX7PIMpQO591EjyinOQGzOMusmlOaC/Zw83/20Dbmz7DKn67yyvUOtkNW5boHRZm1P8gRDWGx5vsjk6rnQCX2gZiO/jnXqbwEVs8xJ65WfafgcPOu2JnjzEX2/E3VehSZm82O9cgeSD4DTr8aguy+bp3P6fpbN50XaSput8GurELW7stl06F8kqNDGdSpDacnR9UYznYieSXlvJi2nbnf76HCbRBss3D9mSm0DbdT7jaocLkpdxlUuN1UuAxcbgO3Yc7/cRsGLgPchoHDZuXs09pycY942kY0ovhBcQ68fAHk7oXTLobR75oV4/IOwIGVsH+VmUVKX2/OQ2ksa5C5yHe77uairl1S4dTBDT+O22UGE988Z/5+6hC4bg5EJpq/l+aZQ8rSK4fxl+ZCymBzPlNC75Pvm53FZsBwYCVEtTczUlWT/N0u87U930BiX3M+UXOv61eSC69cbP7tcuo5MObD6r6yOMecT3Ronfm3y60fVd84LjxsDpFc/Vp1JcGoU+C+DY06Zwp0pEkYhkF6fik7M4tIjgmhU1x4zWoyx+ByG+zNLiIu0kFUyLHvjBiGwWcb03nq400crEy9t4t0EBUSRFRoMJEhwUSGBBEVEnRUkNWG6NDKY1Y4zUovh340Cyj8vNpMpdb34nnLAjjtwmO+/P3OLP7vk81Epi9nnv2p+h2zvsLizPR8ae6J97VHVBaA6G3OJQpvV/mIq35usZrlPrO2mY+qoDB7hzkm22oz5yNZrZU/beZxO50HXYdB5/PrP+GxogxWzzXXIyg4ZG6LOdUMKLO2Ve+XPAAGjjMDH3sD7yxWOL0TeJSXmJNr93xrBjaZm+s+5xEJcNYdZnsj6jnsoxXS9bduOi/SUuzILOTJjzexdNvhkzqO1QIDO8SS2jOBS3ol0LldRJ37udwGpeUuwuy26r8P0jfAK5eYS160P9McZlbVVxwtOBzadTMDlaqApV13M5vhqRSbW10xtqzADALa9TDn1njz5tSWT+CDu83h8hGJ5pzcjI3mzbBjCWtrLtxeVcwhvB04iyofheacIWeR2ddHJpn9ZGhszSU73rkFtn5qZqxuWwTxPWt+Rv5BsyR2SY65OOtl0xr+3XL3m/2fy1ldcdddbv5uGOZ3TehdxyK4LnhrlFmePOoUM0P3y/6x5IgZ7Bxca363y/8K2z+HTR9W/y0WEgMDbjazQW1Pa3j7UaAjLVyxs4J/frmDWd/sotx1/P9ELRY4PTmKwZ3acnbntgzq2KZmtZmKMvPi9PMa8y5MebH5x255iVkau+p5yiAz/VpH4LYjs5Bpn24mbYuZpYh02JifPI+47B/YVxpKjhFJaEw8g07vhiMq3swaOYvMEtzFOZU/s80LExazqk5c1+o1D9p2qR7e5aoMdopzzP2Lc8y7HxkbzDtImZtPXALcPDN4Fm9rDJvdLLPZdRh0vdQc/ldeXHnOis1gqbwYDq4xMzhVK39Hp8CvHoT+o807bHu/hx9mw+aPqi9ywWEQlVy5YG1MzQVsDbeZFSs6XPmz8nlFibnmQ3SKufht9Cnm8+hToE0nM73/yzUSqhgG7F8B694yy2CW5f/iVFnNohrxPc12bf5fdSdsc0Df68whDFWL37nd5r9nYToUZJgZIVeZ+TkYeMZ8GoaZNUzsY5aEPV4nnLPLLG++80szOGzbxXxf1cNHWTxdf+um8yItiWEYLNl6mC+3ZGKxQJDVLIRgs1oIslkJtlqw2SxYLRZsFgsWC+Zzq4XswjLStmTy08Ga183O7cLp2DacgtJy8ksqyC8tp6C0gsLKYXc2q4WY0GCiw4KJCQ3mMvc33JVV/Ue5GxuHI7pxOLoPWTH9KIgbQNuUrvRIiqFNuJ9k07N2wPzRZlBwtKj2ZiCQ2Me8Ibj3O9jzXfWCsA1hj6js1041+7ndX5s3CW9ZAB2G1P2erZ9Vzo8BbnrHXLi2PgozYckz5siLuka/HC06xayY2/1y6HCu2X8dPefqtoVmZcC6lOSa84l+Xl1ze/uB5pIjva856XUWFehIq5Bb7OTAkRIKSivMi2nlz4LSCg7mlrBydw67smpeWCwWaBtuP6qqjfmz6nmwzUKQ1UqQ1UKQrfoiX7WfI8iGI9iK3WbFEWxlf04x7/xwAJfbwGa1cPPgU/nDxV1pG+HAMAzeWrmPqR9twuly0ykunJduPoMeiU3436mrwhz2ll65SGthRmVQcNhMDRcdri4Bbo80AyrPo5sZXDkizDszhtt8uF3mRS//kHmnZtsiOLK7Ye2KTIZfPQADbqk7lV6UZRZx+OG1hh+7Xixm0FiV6Uo43QyCtn0OP75lBhJVolPg9JFm6r9dD/O8BIdUv17hNO8+LZ9h3pWqEtfdvINYlFm54F8D2Oxmu9qfYQ6NS+oHefsrg5u0mu2riz3SDLSiU8xsmD28cihn5fOgULOTdBZX3zUsLzZ/rygxqx41YtKnrr9103mRQPNzbglpmzNYvCmD5buyT3gTsi5XWpfR3pLFWncX1hudKaXuYVdxEQ56JEbSLSGSHomRdGoXTkJkCPFRjjrnJTWpskKz9LRhVN94qmuekavc/MN+11IzWNm/wuyLLVYzmKm6VtvDAYt5c7CojiybxQrX/+fEleI++yOsmGlmke7+DqKSjv8dls0wl8dwFprb2vU0/xawBptzkGx282d5CexbVnONRHukeTN4Z5r5e32q6JXmwVs3mH1on2vhrNu9OnxdgY4EjIz8UpbvymbF7hyW78pm1+FG3FGph9SeCUy6ogen1ZGu/3F/Lr97cw0/55YQEmxl2jV9+HWfZNyGQYXbHPvscptjod1uqHC7K383cFf9NAw6xYUTZj/JhdUMo3IRWGflELZGVDQxDHNRu+2LzKBn7/c1108KCjXvxgSHVi4Me6u5OOzRwcKxuN1mxqI4u47Fa3MBi5kKD29nDuerGpIXEm12Crn7zao4eQcqn+83F9orzjr+5waHm4UR+t9o3p2qT3bEMGD/Slj+LzPL88s7YGFx5rjtiHgzSwVHnW+L+bw0zyxdfqIhidYgc5z3aReZd8myd5njv9M3QOamkxu7DvBoZqPGcuv6WzedFwlk+aXlfLc9i/zScqJCzGHlUaFBnuHlocE2CssqyC0u50ixk9zicvJKnBwpLqe4rIIyl7nuj/Oon4Wl5ew4XMj+nLqrwFaJDAkiISqE+EgH7SIdBNusWDAzUBZL1SXYQpvwYM7t0o6BHWKxB/lgTmuF07yRGOQ4dj9cXlLZl+0zH/k/wyln1S9DU1FmzpVJ32COuOj0q8rFZCsfYW3MG6Nr/wNLppk3RcG80Xbpk+YitcfiLDYr72391KyudvQ84qH3wSVT63cO3JU3U5tgwVgFOhKwDheUkV1UhrPCTbnLTVmFWdXG/N0MNspdBq7KnxUuNxVuw1P9pmr/sgoXzgo3FiyMHNCeIae1Pe7n5hQ5uXfeWr7ZfoI/uI8jyGrh9PbRDOoYy1kd23BWxzbVC7nWoazCxZGicrKLysipLBueXWj+LHe76dIugh6JUXRNiDi5u2DOyiFrwaFmSr2R5SCbVGGmmeHK+KnysdEsTtH+DOh3E/Qcbt69aqy8A5CxCcLbmuO1I+LrX4nHMMws1sG15hDKg+sqK9O0MSfndrnYHNd9rOp0rnIzmEvfYAZ7VeO9nYXV47/LSyqDzzBzCF9w2FHPw82x0I0Yv67rb910XkSaRlFZBdszC9mans/W9EK2ZuSzP6eEjPxSyn5R6bU+IhxBnHNaW87v3o7zu7XjlNiaQ5wNw6Cswk1+aTkYZjbJavXDPq4uWdvNhVnL8mq/FpFoZmmqFm+P7QgXTzEL7TSkD3e7zSHq2xaaQ7nPm2jO6fUxBToiPuByG/z9i228tHTnMdP6QdbKcdHW6vHRVosFt2F4SoEerVtCBF3jIyksqx4DnV9STn5pOaXl9bvoWy3QsW043SvXQ2oX6SAkyEZIsI2QYHO4XkiwlVC7jZgwO7FhwYQG245baKKqc6hwG0Q4vH+3RvyDv1x/Z8yYwbPPPkt6ejr9+vXjH//4B4MGDapz3/fff5+nn36aHTt2UF5eTteuXXnggQe45ZZbPPuMHTuWuXPn1njfsGHDWLhwYb3a4y/nRSRQGIZBfmkFhwtKycwvI6OglKwCJxVuAwMDo7JinGGA24A92UV8ve0w2b/oVzu3CyfMbqscEm8Ohz+6v7bbrCTFhNA+JpT2MaGcEhtG+9hQIkOCag2JD7ZZiAoJ5pTY0HoVZmoShYfNKmyZm6pv9OXurX49tA2c/8dG3+zyVwp0RHyoaq2EqmDGZjUneJ7oLtGBI8Ws2pPDyt1HWLUnhx2ZhSf8LJvVQmxYMG3C7bQJt9M23OGZyLkto4CtGQXkFpef4Ci12YOsxIQGExtmJyYsGAMqJ5qWU1g54bSqc+gcF86gyrUdBnVqU+uO2dEMw6Ck3EVIkK3l3DULYP5w/Z0/fz5jxoxh5syZDB48mOnTp/Puu++ydetW4uPja+2/ZMkSjhw5Qo8ePbDb7Xz88cc88MADfPLJJwwbZg4JGTt2LBkZGbz22mue9zkcDmJjY+vVJn84LyJyfG63wU8H81myNZOl2w6zZt8RjrVmq8VilvCp55quNUSHBtMvJYb+KTH0T4mm3ykxnlLcecXl7DhcwI7MQrZnFLLjcCG5xeWc2yWOK/ok0TMp8phBkmEYbPw5n0U/pbPh5zziIx2c2iaMU9uG0aFtOB3ahBETFlz7/WUFZuGignSzgmpIdMO/lJ9ToCPSCmQXlrFqzxEO5paYZbVDg4mqHAsdFRJcOTY66LgBg2EYZBaUsSW9gG3pZuCTV1JOabmLsnI3pRXVP4vKXOSVOBs1yfRo7WPMdR/ahtvJLnKSVVjmGVaXXVTmOX643Ua4I4gIRxARIUGE24OICQsmLsJBXIQ5/jouwk5cpIN2EQ7aRtgbNIepqoBEfZQ4Xazdd4S1+3MxDIMwu9mucEcQ4Q4bEQ7z/CdFhxB5nBLprY0/XH8HDx7MWWedxT//+U8A3G43KSkp/P73v+eRRx6p1zHOOOMMfv3rX/Pkk08CZqCTm5vLggULGtUmfzgvItIwecXlrN6XgwULkSHVc4oiK/sft2GQUVDGgZxifs4t4ecjJRw4UsLBvBKKKm/uHT0svtzlJre4HKer9uiKU2JDKatwc7jg+FVSO7YN4/I+SVzRO4ne7aNwuQ1W7TnCop/SWbwpg59zTzxnqV2kg9BgG2F2c6RG1fNQu40we2U/VtnfhjtshNvNPjc6NJiYMDvRocGE248/isPfKNARkUYxDINip8szgbTqp9VidgwRIUFEVgYmEY4gXG6D1XuPsHJ3Dit257Dx5zwqGnNLrJ5Cgq2erJWZwbITbLOSW+Ikr6S8ctKr+bOk3EVClINuCZGVjwi6JUTSNcFcH2j13iOsqCxksf5Abr0DvMiQIJKjQ0mOCSE5JpTkmFCiQoI8FfscQVYcwTYcQVaCrFbPnK+j53+VVbhpG+6gW0IEHePCCW7AQoDNydfXX6fTSVhYGO+99x4jR470bL/11lvJzc3lww8/PO77DcPgyy+/5KqrrmLBggVccsklgBnoLFiwALvdTmxsLBdddBFPPfUUbdvWPRevrKyMsrLqP1jy8/NJSUlRvyQS4JwVbrak5/Pj/lzW7c9j3f4j7PxFUaSk6BC6xEd4Ho4gG4s3pbNk6+Ea845OiQ2lqKyCI0eNwggNtnFB93ac0yWO3CIne3OK2ZddzN6cIjLy67PURP0EWS2VgU8wp7WL4PTkaE5PjuL09lEkRoXUGQS53AbZRWXkFpcTH+kgJqz5hsYp0BERnyh2VrB2Xy4rd+dQUu6ibbidtpXZmLhw82d0aDCl5S4Ky8whcIWlFRQ5zfHSucXlZBWWcbig7KifTg4XlNV516yxrJbaQxSSokM4q2MbwuxmxaCisgqKysx2FjkrOFLkJL+0gWWl6yHIaqFTXHhlEBbBKbFhFJaWc6S4nNxis1LRkWIzkAsNttEpLpyOceYaFh3jwujYNrzJSq76+vp78OBB2rdvz/fff8+QIdVrSjz88MMsXbqUFStW1Pm+vLw82rdvT1lZGTabjX/961/cdtttntfnzZtHWFgYnTp1YufOnfzpT38iIiKCZcuWYbPVPpePP/44U6fWrjSkfklEfim/tJyffs4nzG7jtPiIY85jLSqr4KutmXy2IZ0vt2RSUm5W9owJCya1ZwLDTk/kvK5xx7y+lzhd7D9SzJEiJyXlLkqcLvNn5fNip4sip9mXFVf2ZcVO82dBaTl5Jeac3xP1rW3C7fRKiiIuwhylUdU/5xQ5a/SjbcLtdI4L57R2EXRuF07ndhGc2iaM6NBgokODCQm2ei1rpEBHRFoVwzAoLKswh8AVOckpdJJTXFlhrsJNTFgw0WF2YirvSEWHBhNmD2L/kWK2pRewLaOQbRkFbMsoILNyKEFKm1AGdWzL4M5tOLtTW1LanHhCaVFZBYfySvg5t5SDuSUcyjWfFzsrKKvK1pS7Pc8rXEbl+kzWGhmfYJuVQ3mlbM8ooMh5goXb6iE+0lxfomp9KNtRCwIGW638545BOIIaHgz5+vrb2EDH7Xaza9cuCgsLSUtL48knn2TBggVccMEFde6/a9cuTjvtNL744gsuvvjiWq8royMiTanE6WLZrizC7EGc2SGWoGbK8lfNm80rMUdDZBc62Xwon02H8tl0MJ/tmYW4jjNKw2KBSEdQvW4CBtvMrJE5BD+YfqdEM3VE70a1u759k0oliUiLYLFYKsdTB9OhbXi939cu0sEZp9acYJ5b7MRZ4SY+qh7r/vxCuCOILvGRdImPbPB762IYBgfzStmWUcD2DDMgS88rJSo0yFMBzywIYQZxBWXl7M4qZm92EXuyitidVUR+aYUneDsWawsae320uLg4bDYbGRkZNbZnZGSQmJh4zPdZrVa6dOkCQP/+/dm8eTPTpk07ZqDTuXNn4uLi2LFjR52BjsPhwOFo+DpEIiL1EWq3cVGPhGb/XIvFQpg9iDB7EEnRoQAM7RLneb203MX2jEJ+OphHXkn5UfNnHcRF2mkTZifIZqWorILdWUXsPFzIrsNF7MoqYmdmIQfzzEXfXW6DcpdBVqGTrEKzEl6YvenLVCvQEZGA05zjiE/EYrF4yphe2L12BbETMQyD3OJyfs4toayiciFal5tyd/VaUS63QVALrXBnt9sZOHAgaWlpnjk6brebtLQ0JkyYUO/juN3uGhmZXzpw4ADZ2dkkJR1ndXERkQATEmyjzynR9Dnl+JXbwh1B9G4fTe/2tfczDIMip4v8yqyRuURGBeEOBToiInIcFouF2HD7cReWbekmTpzIrbfeyplnnsmgQYOYPn06RUVFjBs3DoAxY8bQvn17pk2bBsC0adM488wzOe200ygrK+PTTz/lP//5Dy+99BIAhYWFTJ06ld/85jckJiayc+dOHn74Ybp06eIpPy0iIt5hsVjM6qqOIJJjQpv1sxXoiIiIXxs1ahSHDx9m8uTJpKen079/fxYuXEhCgjnMY9++fVit1ePZi4qK+N3vfseBAwcIDQ2lR48evPHGG4waNQoAm83G+vXrmTt3Lrm5uSQnJ3PppZfy5JNPaniaiEgromIEIiJyTLr+1k3nRUTEd+p7DfbPhRtEREREREROggIdERERERFpdRToiIiIiIhIq6NAR0REREREWh0FOiIiIiIi0uoo0BERERERkVZHgY6IiIiIiLQ6CnRERERERKTVCfJ1A+qjak3T/Px8H7dERCSwVF13W8Da0s1K/ZKIiO/Ut29qEYFOQUEBACkpKT5uiYhIYCooKCA6OtrXzfAb6pdERHzvRH2TxWgBt+ncbjcHDx4kMjISi8XS4Pfn5+eTkpLC/v37iYqKaoIW+j+dA52DKjoPOgdV6nMeDMOgoKCA5ORkrFaNdq6ifsk7dB50DqroPOgcQP3PQX37phaR0bFarZxyyiknfZyoqKiA/Q+nis6BzkEVnQedgyonOg/K5NSmfsm7dB50DqroPOgcQP3OQX36Jt2eExERERGRVkeBjoiIiIiItDoBEeg4HA6mTJmCw+HwdVN8RudA56CKzoPOQRWdB9/RuTfpPOgcVNF50DkA75+DFlGMQEREREREpCECIqMjIiIiIiKBRYGOiIiIiIi0Ogp0RERERESk1VGgIyIiIiIirU6rD3RmzJhBx44dCQkJYfDgwaxcudLXTWpSX3/9NcOHDyc5ORmLxcKCBQtqvG4YBpMnTyYpKYnQ0FBSU1PZvn27bxrbRKZNm8ZZZ51FZGQk8fHxjBw5kq1bt9bYp7S0lHvuuYe2bdsSERHBb37zGzIyMnzUYu976aWX6Nu3r2fBrSFDhvDZZ595Xm/t378uzzzzDBaLhfvuu8+zLRDOw+OPP47FYqnx6NGjh+f1QDgH/kh904Iar7f2vkn9kkl9U23qm5q2b2rVgc78+fOZOHEiU6ZMYc2aNfTr149hw4aRmZnp66Y1maKiIvr168eMGTPqfP2vf/0rL774IjNnzmTFihWEh4czbNgwSktLm7mlTWfp0qXcc889LF++nMWLF1NeXs6ll15KUVGRZ5/777+f//3vf7z77rssXbqUgwcPcs011/iw1d51yimn8Mwzz7B69Wp++OEHLrroIkaMGMFPP/0EtP7v/0urVq3i3//+N3379q2xPVDOw+mnn86hQ4c8j2+//dbzWqCcA3+ivqm21t43qV8yqW+qSX1TM/RNRis2aNAg45577vH87nK5jOTkZGPatGk+bFXzAYwPPvjA87vb7TYSExONZ5991rMtNzfXcDgcxttvv+2DFjaPzMxMAzCWLl1qGIb5nYODg413333Xs8/mzZsNwFi2bJmvmtnkYmNjjVdeeSXgvn9BQYHRtWtXY/Hixcb5559v3HvvvYZhBM5/B1OmTDH69etX52uBcg78jfom9U3ql6qpb1Lf9EvePAetNqPjdDpZvXo1qampnm1Wq5XU1FSWLVvmw5b5zu7du0lPT69xTqKjoxk8eHCrPid5eXkAtGnTBoDVq1dTXl5e4zz06NGDU089tVWeB5fLxbx58ygqKmLIkCEB9/3vuecefv3rX9f4vhBY/x1s376d5ORkOnfuzOjRo9m3bx8QWOfAX6hvqi0Q+6ZA75dAfZP6pubpm4K82mI/kpWVhcvlIiEhocb2hIQEtmzZ4qNW+VZ6ejpAneek6rXWxu12c9999zF06FB69+4NmOfBbrcTExNTY9/Wdh42bNjAkCFDKC0tJSIigg8++IBevXqxbt26gPj+APPmzWPNmjWsWrWq1muB8t/B4MGDmTNnDt27d+fQoUNMnTqV8847j40bNwbMOfAn6ptqC7S+KZD7JVDfBOqboPn6plYb6IiAecdk48aNNcZ9Boru3buzbt068vLyeO+997j11ltZunSpr5vVbPbv38+9997L4sWLCQkJ8XVzfObyyy/3PO/bty+DBw+mQ4cOvPPOO4SGhvqwZSKBKZD7JVDfpL7J1Fx9U6sduhYXF4fNZqtVoSEjI4PExEQftcq3qr53oJyTCRMm8PHHH/PVV19xyimneLYnJibidDrJzc2tsX9rOw92u50uXbowcOBApk2bRr9+/fj73/8eMN9/9erVZGZmcsYZZxAUFERQUBBLly7lxRdfJCgoiISEhIA4D78UExNDt27d2LFjR8D8t+BP1DfVFkh9U6D3S6C+SX1T3Zqqb2q1gY7dbmfgwIGkpaV5trndbtLS0hgyZIgPW+Y7nTp1IjExscY5yc/PZ8WKFa3qnBiGwYQJE/jggw/48ssv6dSpU43XBw4cSHBwcI3zsHXrVvbt29eqzsMvud1uysrKAub7X3zxxWzYsIF169Z5HmeeeSajR4/2PA+E8/BLhYWF7Ny5k6SkpID5b8GfqG+qLRD6JvVLx6a+SX0TNGHf1Ph6Cf5v3rx5hsPhMObMmWNs2rTJuOuuu4yYmBgjPT3d101rMgUFBcbatWuNtWvXGoDx/PPPG2vXrjX27t1rGIZhPPPMM0ZMTIzx4YcfGuvXrzdGjBhhdOrUySgpKfFxy71n/PjxRnR0tLFkyRLj0KFDnkdxcbFnn7vvvts49dRTjS+//NL44YcfjCFDhhhDhgzxYau965FHHjGWLl1q7N6921i/fr3xyCOPGBaLxfj8888Nw2j93/9Yjq5sYxiBcR4eeOABY8mSJcbu3buN7777zkhNTTXi4uKMzMxMwzAC4xz4G/VNgdc3qV8yqW+qm/qmpuubWnWgYxiG8Y9//MM49dRTDbvdbgwaNMhYvny5r5vUpL766isDqPW49dZbDcMwy3g+9thjRkJCguFwOIyLL77Y2Lp1q28b7WV1fX/AeO211zz7lJSUGL/73e+M2NhYIywszLj66quNQ4cO+a7RXnbbbbcZHTp0MOx2u9GuXTvj4osv9nQkhtH6v/+x/LIzCYTzMGrUKCMpKcmw2+1G+/btjVGjRhk7duzwvB4I58AfqW8KrL5J/ZJJfVPd1Dc1Xd9kMQzDaGSWSURERERExC+12jk6IiIiIiISuBToiIiIiIhIq6NAR0REREREWh0FOiIiIiIi0uoo0BERERERkVZHgY6IiIiIiLQ6CnRERERERKTVUaAjIiIiIiKtjgIdERERERFpdRToiIiIiIhIq6NAR0REREREWh0FOiIiIiIi0ur8P17TNufxTEx3AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "RMSE: 0.5313848379900923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS PRAKTIKUM"
      ],
      "metadata": {
        "id": "P4iKOdODD6uX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksekusi Kode Tugas Praktikum"
      ],
      "metadata": {
        "id": "GSMg2SVzE0mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 → 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Bangun model JST (MLP)\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),         # Ubah gambar 28x28 → vektor 784\n",
        "    Dense(128, activation='relu'),         # Hidden layer 1 (128 neuron)\n",
        "    Dense(64, activation='relu'),          # Hidden layer 2 (64 neuron)\n",
        "    Dense(10, activation='softmax')        # Output (10 kelas)\n",
        "])\n",
        "\n",
        "# 3. Kompilasi model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Latih model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# 5. Evaluasi model\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "UIhgiNDqEF4b",
        "outputId": "cf051ae5-981e-447c-be37-08c14a777d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8737 - loss: 0.4256\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9675 - loss: 0.1078\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9787 - loss: 0.0719\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9841 - loss: 0.0514\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9850 - loss: 0.0441\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9727 - loss: 0.0948\n",
            "Akurasi pada data uji: 0.9766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soal Tugas Praktikum"
      ],
      "metadata": {
        "id": "yGYRLeGdE7g0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 1 : Ubah jumlah neuron di hidden layer (misal: 256 dan 128)"
      ],
      "metadata": {
        "id": "tztZXFZQFBsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Bangun model JST dengan neuron 256 dan 128\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),      # input 784\n",
        "    Dense(256, activation='relu'),      # Hidden layer 1 (256 neuron)\n",
        "    Dense(128, activation='relu'),      # Hidden layer 2 (128 neuron)\n",
        "    Dense(10, activation='softmax')     # Output layer\n",
        "])\n",
        "\n",
        "# 3. Kompilasi\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Training\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# 5. Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "KMVd26cTEwYe",
        "outputId": "42d4a8f1-b27e-4994-c259-2533b7128ab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.8892 - loss: 0.3625\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9737 - loss: 0.0849\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9828 - loss: 0.0561\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9867 - loss: 0.0415\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9888 - loss: 0.0327\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9739 - loss: 0.0916\n",
            "Akurasi pada data uji: 0.9777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 2 : Tambahkan satu hidden layer lagi"
      ],
      "metadata": {
        "id": "2RlctEAqFKsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (agar nilai piksel berada pada rentang 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding (label 0-9 diubah menjadi vektor 10 dimensi)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Bangun model JST dengan 3 hidden layer\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),        # Mengubah input gambar 28x28 menjadi vektor 784\n",
        "    Dense(256, activation='relu'),        # Hidden layer 1 (256 neuron, ReLU)\n",
        "    Dense(128, activation='relu'),        # Hidden layer 2 (128 neuron, ReLU)\n",
        "    Dense(64, activation='relu'),         # Hidden layer 3 (baru ditambahkan, 64 neuron)\n",
        "    Dense(10, activation='softmax')       # Output layer (10 kelas digit MNIST)\n",
        "])\n",
        "\n",
        "# 3. Kompilasi model (mengatur optimizer, loss function, dan metrik evaluasi)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Training (melatih model selama 5 epoch)\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# 5. Evaluasi model pada data uji\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "7qIQhcAMGptY",
        "outputId": "04afff63-f140-4e36-b1af-42b88532f3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.8853 - loss: 0.3781\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9717 - loss: 0.0914\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9806 - loss: 0.0600\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9840 - loss: 0.0486\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9882 - loss: 0.0371\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9771 - loss: 0.0907\n",
            "Akurasi pada data uji: 0.9800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 3 : Bandingkan akurasi dan waktu pelatihan"
      ],
      "metadata": {
        "id": "C_CyakgBFMrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah dibandingkan, penambahan satu hidden layer (dari 2 menjadi 3 hidden layer) menghasilkan akurasi yang sedikit lebih tinggi, yaitu dari 0.9777 → 0.9800, sehingga model menjadi sedikit lebih baik dalam mengenali digit MNIST. Namun, waktu pelatihannya menjadi lebih lama, terlihat dari durasi per epoch yang meningkat (model awal ±11–12 detik/epoch, sedangkan model dengan tambahan layer mencapai ±12–16 detik/epoch). Hal ini wajar karena jumlah parameter bertambah, sehingga komputasi semakin besar. Kesimpulannya, menambah hidden layer dapat meningkatkan akurasi, tetapi juga menambah waktu pelatihan."
      ],
      "metadata": {
        "id": "EmOcmMoBIKbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOAL 4 : Eksperimen dengan fungsi aktivasi Sigmoid vs ReLU"
      ],
      "metadata": {
        "id": "w18TgEMmFNw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Fungsi untuk membangun dan melatih model\n",
        "def train_model(activation):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(256, activation=activation),\n",
        "        Dense(128, activation=activation),\n",
        "        Dense(64, activation=activation),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, y_test)\n",
        "    return acc, elapsed_time\n",
        "\n",
        "# 2. Latih dengan ReLU\n",
        "acc_relu, time_relu = train_model('relu')\n",
        "print(f\"\\nReLU → Akurasi: {acc_relu:.4f}, Waktu pelatihan: {time_relu:.2f} detik\")\n",
        "\n",
        "# 3. Latih dengan Sigmoid\n",
        "acc_sigmoid, time_sigmoid = train_model('sigmoid')\n",
        "print(f\"\\nSigmoid → Akurasi: {acc_sigmoid:.4f}, Waktu pelatihan: {time_sigmoid:.2f} detik\")"
      ],
      "metadata": {
        "id": "4Dy-Cm5FIkUZ",
        "outputId": "b3ebe41e-24ff-41d8-e3d1-031ae00cbc0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.8810 - loss: 0.3886\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9693 - loss: 0.0997\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9791 - loss: 0.0666\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.0482\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.9868 - loss: 0.0393\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9728 - loss: 0.1002\n",
            "\n",
            "ReLU → Akurasi: 0.9751, Waktu pelatihan: 70.68 detik\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7442 - loss: 0.8937\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9509 - loss: 0.1668\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.9687 - loss: 0.1054\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9757 - loss: 0.0769\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.9825 - loss: 0.0596\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9701 - loss: 0.0896\n",
            "\n",
            "Sigmoid → Akurasi: 0.9745, Waktu pelatihan: 77.43 detik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari eksperimen, ReLU memberikan akurasi sedikit lebih tinggi (0.9751 vs 0.9745) dan waktu pelatihan lebih cepat (70.68s vs 77.43s) dibanding Sigmoid, karena ReLU mengurangi vanishing gradient dan mempercepat konvergensi, sedangkan Sigmoid cenderung memperlambat pelatihan. Secara keseluruhan, ReLU lebih efisien."
      ],
      "metadata": {
        "id": "rxD9KgU9IPUq"
      }
    }
  ]
}